----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
          Dropout-45           [-1, 512, 8, 13]               0
           Conv2d-46           [-1, 256, 8, 13]       1,179,904
             ReLU-47           [-1, 256, 8, 13]               0
      BatchNorm2d-48           [-1, 256, 8, 13]             512
           Conv2d-49           [-1, 256, 8, 13]         590,080
             ReLU-50           [-1, 256, 8, 13]               0
      BatchNorm2d-51           [-1, 256, 8, 13]             512
           Conv2d-52           [-1, 256, 8, 13]         590,080
             ReLU-53           [-1, 256, 8, 13]               0
      BatchNorm2d-54           [-1, 256, 8, 13]             512
           Conv2d-55            [-1, 256, 4, 7]         590,080
             ReLU-56            [-1, 256, 4, 7]               0
      BatchNorm2d-57            [-1, 256, 4, 7]             512
             View-58                 [-1, 7168]               0
           Linear-59                    [-1, 4]          28,676
          Sigmoid-60                    [-1, 4]               0
  ConvTranspose2d-61          [-1, 512, 16, 26]       1,049,088
           Conv2d-62          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-63          [-1, 256, 17, 26]             512
             ReLU-64          [-1, 256, 17, 26]               0
           Conv2d-65          [-1, 256, 17, 26]         590,080
      BatchNorm2d-66          [-1, 256, 17, 26]             512
             ReLU-67          [-1, 256, 17, 26]               0
      double_conv-68          [-1, 256, 17, 26]               0
               up-69          [-1, 256, 17, 26]               0
  ConvTranspose2d-70          [-1, 256, 34, 52]         262,400
           Conv2d-71          [-1, 128, 35, 52]         589,952
      BatchNorm2d-72          [-1, 128, 35, 52]             256
             ReLU-73          [-1, 128, 35, 52]               0
           Conv2d-74          [-1, 128, 35, 52]         147,584
      BatchNorm2d-75          [-1, 128, 35, 52]             256
             ReLU-76          [-1, 128, 35, 52]               0
      double_conv-77          [-1, 128, 35, 52]               0
               up-78          [-1, 128, 35, 52]               0
  ConvTranspose2d-79         [-1, 128, 70, 104]          65,664
           Conv2d-80          [-1, 64, 70, 105]         147,520
      BatchNorm2d-81          [-1, 64, 70, 105]             128
             ReLU-82          [-1, 64, 70, 105]               0
           Conv2d-83          [-1, 64, 70, 105]          36,928
      BatchNorm2d-84          [-1, 64, 70, 105]             128
             ReLU-85          [-1, 64, 70, 105]               0
      double_conv-86          [-1, 64, 70, 105]               0
               up-87          [-1, 64, 70, 105]               0
  ConvTranspose2d-88         [-1, 64, 140, 210]          16,448
           Conv2d-89         [-1, 64, 140, 210]          73,792
      BatchNorm2d-90         [-1, 64, 140, 210]             128
             ReLU-91         [-1, 64, 140, 210]               0
           Conv2d-92         [-1, 64, 140, 210]          36,928
      BatchNorm2d-93         [-1, 64, 140, 210]             128
             ReLU-94         [-1, 64, 140, 210]               0
      double_conv-95         [-1, 64, 140, 210]               0
               up-96         [-1, 64, 140, 210]               0
           Conv2d-97          [-1, 4, 140, 210]             260
          outconv-98          [-1, 4, 140, 210]               0
================================================================
Total params: 17,769,992
Trainable params: 17,769,992
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 419.87
Params size (MB): 67.79
Estimated Total Size (MB): 488.00
----------------------------------------------------------------
Learning rate: 0.0004
2019-12-13 02:11:32.126745
Classifier weightL: 0.2
Using Dice loss
Reading data
Loaded data
Training model
Dice: 8.2310670757215e-05
Loss: 0.06344271883110693
Err: [0.51206434 0.54021448 0.52815013 0.70509383]
Epoch 1 loss = -0.2208502182799081
-------------
Dice: 0.29529901407190556
Loss: -0.7082999974858121
Err: [0.48525469 0.48793566 0.50938338 0.41286863]
Epoch 2 loss = -0.6206692708656192
-------------
Dice: 0.13191220393164224
Loss: -0.07290989884567677
Err: [0.51742627 0.52010724 0.50670241 0.32037534]
Epoch 3 loss = -0.1773693369856725
-------------
Dice: 0.22346340054854807
Loss: -0.4533653708131754
Err: [0.43565684 0.45576408 0.49865952 0.46514745]
Epoch 4 loss = -0.2993120543162028
-------------
Dice: 0.28300658376705895
Loss: -0.699304768488043
Err: [0.49463807 0.47184987 0.46380697 0.4155496 ]
Epoch 5 loss = -0.6196412285355231
-------------
Dice: 0.21254988661080493
Loss: 0.659815484591176
Err: [0.49731903 0.46380697 0.51474531 0.45978552]
Epoch 6 loss = -0.21613695090947052
-------------
Dice: 8.2310670757215e-05
Loss: -0.44222574116609265
Err: [0.53619303 0.46514745 0.47453083 0.37399464]
Epoch 7 loss = -0.2948401713371277
-------------
Dice: 8.2310670757215e-05
Loss: -0.6247878224625135
Err: [0.49463807 0.45710456 0.5080429  0.42493298]
Epoch 8 loss = -0.4124563507984082
-------------
Dice: 0.0007769247318299617
Loss: -0.42915986011294155
Err: [0.5080429  0.4919571  0.49061662 0.3230563 ]
Epoch 9 loss = -0.4766429214614133
-------------
Dice: 0.1029239468354205
Loss: -0.18532958646655556
Err: [0.48391421 0.4772118  0.52144772 0.39008043]
Epoch 10 loss = -0.09205615904182196
-------------
Dice: 0.23521343452313742
Loss: -0.1671701742797446
Err: [0.48793566 0.46246649 0.48391421 0.35790885]
Epoch 11 loss = -0.201975307604298
-------------
Dice: 0.2716403869130356
Loss: -0.40052832864115595
Err: [0.51340483 0.47587131 0.45576408 0.34718499]
Epoch 12 loss = -0.38317062785228095
-------------
Dice: 0.23006441335712513
Loss: 0.031219760001569626
Err: [0.48123324 0.45174263 0.52546917 0.31903485]
Epoch 13 loss = -0.21062003367270032
-------------
Dice: 0.28000077123993494
Loss: -0.48406938019417095
Err: [0.51608579 0.43967828 0.49463807 0.33243968]
Epoch 14 loss = -0.3560550639716287
-------------
Dice: 0.2923367973352765
Loss: -0.6476433123850474
Err: [0.48927614 0.43565684 0.5227882  0.36595174]
Epoch 15 loss = -0.5230673341825605
-------------
Dice: 0.2438830422596729
Loss: -0.2833368784434976
Err: [0.51742627 0.49463807 0.50134048 0.36461126]
Epoch 16 loss = -0.29476114988016583
-------------
Dice: 0.21246700555141299
Loss: -1.1803827865818315
Err: [0.47587131 0.45844504 0.5080429  0.33512064]
Epoch 17 loss = 0.25338457581587137
-------------
Dice: 0.11719633027003953
Loss: -0.12089600729734287
Err: [0.49731903 0.44369973 0.4919571  0.3150134 ]
Epoch 18 loss = 0.002273471215739846
-------------
Dice: 0.12473775586638605
Loss: -0.1649946319083677
Err: [0.47989276 0.44906166 0.50268097 0.31367292]
Epoch 19 loss = -0.03708263480725388
-------------
Dice: 0.12498861665234118
Loss: -0.19588685740525436
Err: [0.5308311  0.46246649 0.50670241 0.3150134 ]
Epoch 20 loss = -0.07961677291740973
-------------
Dice: 0.12559930241190598
Loss: -0.21826945865528669
Err: [0.51608579 0.48793566 0.49865952 0.32439678]
Epoch 21 loss = -0.20106583331401148
-------------
Dice: 0.21767152878101412
Loss: -0.4224473066768349
Err: [0.52412869 0.47989276 0.52815013 0.32975871]
Epoch 22 loss = -0.4113838495438298
-------------
Dice: 0.20499830937436558
Loss: -0.5171580306780342
Err: [0.49865952 0.48659517 0.48793566 0.34718499]
Epoch 23 loss = -0.6716109644528478
-------------
Dice: 0.13917806385532375
Loss: -0.10984449443088151
Err: [0.49865952 0.46514745 0.4691689  0.34048257]
Epoch 24 loss = -0.026861478084077437
-------------
Dice: 0.15770373317152056
Loss: -0.17921295411803714
Err: [0.48123324 0.46648794 0.49731903 0.36595174]
Epoch 25 loss = -0.08881206101737917
-------------
Dice: 0.16505801385945593
Loss: -0.23777090419799063
Err: [0.4919571  0.47319035 0.49731903 0.28954424]
Epoch 26 loss = -0.17957598503679037
-------------
Dice: 0.2026408405734362
Loss: -0.33240671918673464
Err: [0.48391421 0.47050938 0.48123324 0.33512064]
Epoch 27 loss = -0.3272197692996512
-------------
Dice: 0.14491300720050682
Loss: -0.7226014981917308
Err: [0.50402145 0.50938338 0.48927614 0.44235925]
Epoch 28 loss = -0.19938239546492695
-------------
Dice: 0.1312398083521344
Loss: -0.24499127397642947
Err: [0.50268097 0.45040214 0.45978552 0.33243968]
Epoch 29 loss = -0.35650444868331155
-------------
Dice: 0.19847549519112484
Loss: -0.4901901394094643
Err: [0.51742627 0.46782842 0.5080429  0.37399464]
Epoch 30 loss = -0.023961889470616976
-------------
Dice: 0.054078937647313155
Loss: -0.37953439782231285
Err: [0.50536193 0.48793566 0.54289544 0.34182306]
Epoch 31 loss = -0.2465060933244725
-------------
Dice: 0.06329542589053709
Loss: -0.3350136720494401
Err: [0.51608579 0.46246649 0.50268097 0.29356568]
Epoch 32 loss = -0.2563686274923384
-------------
Dice: 0.05089709902410178
Loss: -0.4327730729210942
Err: [0.51742627 0.45844504 0.49731903 0.30831099]
Epoch 33 loss = -0.2962834540506204
-------------
Dice: 0.015613331394084705
Loss: -0.5847031899113657
Err: [0.50134048 0.46782842 0.5        0.33914209]
Epoch 34 loss = -0.3552509520513316
-------------
Dice: 0.03995070879487428
Loss: -5.505907641372127
Err: [0.49731903 0.46514745 0.50268097 0.44235925]
Epoch 35 loss = -0.43203529187788564
-------------
Dice: 0.10197345110957798
Loss: 0.08177895930083805
Err: [0.52010724 0.50268097 0.48927614 0.33378016]
Epoch 36 loss = 0.10229422569585343
-------------
Dice: 0.10447308724574009
Loss: -0.044693569047017594
Err: [0.50402145 0.46648794 0.49865952 0.34718499]
Epoch 37 loss = 0.04933081475396951
-------------
Dice: 0.11216003164950737
Loss: -0.06553524104118154
Err: [0.49597855 0.45442359 0.46782842 0.34450402]
Epoch 38 loss = 0.0028052502777427433
-------------
Dice: 0.1315816181684311
Loss: -0.15198374250106686
Err: [0.52680965 0.45978552 0.51072386 0.37399464]
Epoch 39 loss = -0.22775864092633127
-------------
Dice: 0.15427063565056567
Loss: -0.17188389655379602
Err: [0.51206434 0.46112601 0.52010724 0.31769437]
Epoch 40 loss = -0.13076850860379635
-------------
