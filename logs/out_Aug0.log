----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
  ConvTranspose2d-45          [-1, 512, 16, 26]       1,049,088
           Conv2d-46          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-47          [-1, 256, 17, 26]             512
             ReLU-48          [-1, 256, 17, 26]               0
           Conv2d-49          [-1, 256, 17, 26]         590,080
      BatchNorm2d-50          [-1, 256, 17, 26]             512
             ReLU-51          [-1, 256, 17, 26]               0
      double_conv-52          [-1, 256, 17, 26]               0
               up-53          [-1, 256, 17, 26]               0
  ConvTranspose2d-54          [-1, 256, 34, 52]         262,400
           Conv2d-55          [-1, 128, 35, 52]         589,952
      BatchNorm2d-56          [-1, 128, 35, 52]             256
             ReLU-57          [-1, 128, 35, 52]               0
           Conv2d-58          [-1, 128, 35, 52]         147,584
      BatchNorm2d-59          [-1, 128, 35, 52]             256
             ReLU-60          [-1, 128, 35, 52]               0
      double_conv-61          [-1, 128, 35, 52]               0
               up-62          [-1, 128, 35, 52]               0
  ConvTranspose2d-63         [-1, 128, 70, 104]          65,664
           Conv2d-64          [-1, 64, 70, 105]         147,520
      BatchNorm2d-65          [-1, 64, 70, 105]             128
             ReLU-66          [-1, 64, 70, 105]               0
           Conv2d-67          [-1, 64, 70, 105]          36,928
      BatchNorm2d-68          [-1, 64, 70, 105]             128
             ReLU-69          [-1, 64, 70, 105]               0
      double_conv-70          [-1, 64, 70, 105]               0
               up-71          [-1, 64, 70, 105]               0
  ConvTranspose2d-72         [-1, 64, 140, 210]          16,448
           Conv2d-73         [-1, 64, 140, 210]          73,792
      BatchNorm2d-74         [-1, 64, 140, 210]             128
             ReLU-75         [-1, 64, 140, 210]               0
           Conv2d-76         [-1, 64, 140, 210]          36,928
      BatchNorm2d-77         [-1, 64, 140, 210]             128
             ReLU-78         [-1, 64, 140, 210]               0
      double_conv-79         [-1, 64, 140, 210]               0
               up-80         [-1, 64, 140, 210]               0
           Conv2d-81          [-1, 4, 140, 210]             260
          outconv-82          [-1, 4, 140, 210]               0
================================================================
Total params: 14,789,124
Trainable params: 14,789,124
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 417.42
Params size (MB): 56.42
Estimated Total Size (MB): 474.17
----------------------------------------------------------------
Learning rate: 0.0004
Augmentation: 0
Using distance weighted Dice
Reading data
Loaded data
calculating weights
Training model
Dice: 0.21286093081097812
Loss: -0.6679353412770075
Epoch 1 loss = -0.7353977279489239
-------------
Dice: 0.3818017592427843
Loss: -0.581846520630674
Epoch 2 loss = -0.8474480881169438
-------------
Dice: 0.44436398786111264
Loss: -0.6568399277612774
Epoch 3 loss = -0.8898147609705727
-------------
Dice: 0.44621127404755995
Loss: -0.6564770992245275
Epoch 4 loss = -0.9241837172582745
-------------
Dice: 0.44357779432573746
Loss: -0.6547205282067832
Epoch 5 loss = -0.9425911763434609
-------------
Dice: 0.4938097563599669
Loss: -0.6856537318403643
Epoch 6 loss = -0.9475172053153316
-------------
Dice: 0.505377290983143
Loss: -0.7656522727131242
Epoch 7 loss = -0.951577949275573
-------------
Dice: 0.49638372670906
Loss: -0.7496536164442114
Epoch 8 loss = -0.9584749012130002
-------------
Dice: 0.5010186179308497
Loss: -0.7194087931085092
Epoch 9 loss = -0.9772088075677554
-------------
Dice: 0.4997845523439404
Loss: -0.7244212855759309
Epoch 10 loss = -0.9744965738430619
-------------
Dice: 0.5141610944297732
Loss: -0.7159017793197787
Epoch 11 loss = -0.9695362118134896
-------------
Dice: 0.5063300273604145
Loss: -0.7494893512597208
Epoch 12 loss = -0.9822305648960173
-------------
Dice: 0.519290394259486
Loss: -0.7649060214414434
Epoch 13 loss = -0.979648729916662
-------------
Dice: 0.5106514258076643
Loss: -0.7356698567010224
Epoch 14 loss = -0.99026713060836
-------------
Dice: 0.5292288641137708
Loss: -0.7442352980743945
Epoch 15 loss = -1.0045597587525845
-------------
Dice: 0.5189192537941936
Loss: -0.7284567639121792
Epoch 16 loss = -0.9870204491044084
-------------
Dice: 0.520000900551898
Loss: -0.7470979014341423
Epoch 17 loss = -1.0072205562889576
-------------
Dice: 0.5293499255900246
Loss: -0.7385778232066523
Epoch 18 loss = -1.0120499132821956
-------------
Dice: 0.5327558814574958
Loss: -0.7397031839001533
Epoch 19 loss = -1.0269533630087972
-------------
Dice: 0.5174338666164322
Loss: -0.7349614635365809
Epoch 20 loss = -1.0229073289533457
-------------
Dice: 0.5232855258933142
Loss: -0.6867967730720252
Epoch 21 loss = -1.0268306992575527
-------------
Dice: 0.5176735283500321
Loss: -0.7205313169095744
Epoch 22 loss = -1.0311188515648246
-------------
Dice: 0.5258038730917709
Loss: -0.7081619270776873
Epoch 23 loss = -1.0283014302949112
-------------
Dice: 0.5214574272103684
Loss: -0.7228923455863964
Epoch 24 loss = -1.038261204957962
-------------
Dice: 0.5161099680871666
Loss: -0.697811542136992
Epoch 25 loss = -1.0426396812995276
-------------
Dice: 0.5382700949200643
Loss: -0.7343333264175039
Epoch 26 loss = -1.0438242480407158
-------------
Dice: 0.5274960212545096
Loss: -0.7546871938064765
Epoch 27 loss = -1.0485502994929752
-------------
Dice: 0.5302161562807776
Loss: -0.7163382418782896
Epoch 28 loss = -1.060159874757131
-------------
Dice: 0.5284430117063809
Loss: -0.6943107602002476
Epoch 29 loss = -1.058644062379996
-------------
Dice: 0.5149842045078947
Loss: -0.6779984064670138
Epoch 30 loss = -1.0672056931629776
-------------
Dice: 0.5306135816116682
Loss: -0.73677309900824
Epoch 31 loss = -1.0592984736338258
-------------
Dice: 0.5386115585112161
Loss: -0.7108509681837988
Epoch 32 loss = -1.0784931622942289
-------------
Dice: 0.5373819615502127
Loss: -0.7428635239340469
Epoch 33 loss = -1.0733422318845987
-------------
Dice: 0.4784332075770512
Loss: -0.722865667269521
Epoch 34 loss = -1.0817770467946928
-------------
Dice: 0.5446978280202157
Loss: -0.7475082016287036
Epoch 35 loss = -1.0839258862783512
-------------
Dice: 0.5318267756403375
Loss: -0.7140556885083328
Epoch 36 loss = -1.071035008256634
-------------
Dice: 0.5436720722478814
Loss: -0.7299655750120385
Epoch 37 loss = -1.0849199402580658
-------------
Dice: 0.5215892579189908
Loss: -0.7258338071139379
Epoch 38 loss = -1.100243197878202
-------------
Dice: 0.5301044755375501
Loss: -0.7137581432083212
Epoch 39 loss = -1.0880709586540858
-------------
Dice: 0.5425990123263817
Loss: -0.7208437170360724
Epoch 40 loss = -1.1003526196504632
-------------
