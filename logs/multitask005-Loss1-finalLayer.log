----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
  ConvTranspose2d-45          [-1, 512, 16, 26]       1,049,088
           Conv2d-46          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-47          [-1, 256, 17, 26]             512
             ReLU-48          [-1, 256, 17, 26]               0
           Conv2d-49          [-1, 256, 17, 26]         590,080
      BatchNorm2d-50          [-1, 256, 17, 26]             512
             ReLU-51          [-1, 256, 17, 26]               0
      double_conv-52          [-1, 256, 17, 26]               0
               up-53          [-1, 256, 17, 26]               0
  ConvTranspose2d-54          [-1, 256, 34, 52]         262,400
           Conv2d-55          [-1, 128, 35, 52]         589,952
      BatchNorm2d-56          [-1, 128, 35, 52]             256
             ReLU-57          [-1, 128, 35, 52]               0
           Conv2d-58          [-1, 128, 35, 52]         147,584
      BatchNorm2d-59          [-1, 128, 35, 52]             256
             ReLU-60          [-1, 128, 35, 52]               0
      double_conv-61          [-1, 128, 35, 52]               0
               up-62          [-1, 128, 35, 52]               0
  ConvTranspose2d-63         [-1, 128, 70, 104]          65,664
           Conv2d-64          [-1, 64, 70, 105]         147,520
      BatchNorm2d-65          [-1, 64, 70, 105]             128
             ReLU-66          [-1, 64, 70, 105]               0
           Conv2d-67          [-1, 64, 70, 105]          36,928
      BatchNorm2d-68          [-1, 64, 70, 105]             128
             ReLU-69          [-1, 64, 70, 105]               0
      double_conv-70          [-1, 64, 70, 105]               0
               up-71          [-1, 64, 70, 105]               0
  ConvTranspose2d-72         [-1, 64, 140, 210]          16,448
           Conv2d-73         [-1, 64, 140, 210]          73,792
      BatchNorm2d-74         [-1, 64, 140, 210]             128
             ReLU-75         [-1, 64, 140, 210]               0
           Conv2d-76         [-1, 64, 140, 210]          36,928
      BatchNorm2d-77         [-1, 64, 140, 210]             128
             ReLU-78         [-1, 64, 140, 210]               0
      double_conv-79         [-1, 64, 140, 210]               0
               up-80         [-1, 64, 140, 210]               0
          Dropout-81         [-1, 64, 140, 210]               0
           Conv2d-82         [-1, 128, 70, 105]          73,856
             ReLU-83         [-1, 128, 70, 105]               0
      BatchNorm2d-84         [-1, 128, 70, 105]             256
           Conv2d-85          [-1, 128, 35, 53]         147,584
             ReLU-86          [-1, 128, 35, 53]               0
      BatchNorm2d-87          [-1, 128, 35, 53]             256
           Conv2d-88          [-1, 256, 35, 53]         295,168
             ReLU-89          [-1, 256, 35, 53]               0
      BatchNorm2d-90          [-1, 256, 35, 53]             512
        AvgPool2d-91           [-1, 256, 8, 13]               0
           Conv2d-92           [-1, 256, 8, 13]         590,080
             ReLU-93           [-1, 256, 8, 13]               0
      BatchNorm2d-94           [-1, 256, 8, 13]             512
           Conv2d-95           [-1, 256, 8, 13]         590,080
             ReLU-96           [-1, 256, 8, 13]               0
      BatchNorm2d-97           [-1, 256, 8, 13]             512
           Conv2d-98            [-1, 256, 4, 7]         590,080
             ReLU-99            [-1, 256, 4, 7]               0
     BatchNorm2d-100            [-1, 256, 4, 7]             512
            View-101                 [-1, 7168]               0
          Linear-102                    [-1, 4]          28,676
         Sigmoid-103                    [-1, 4]               0
          Conv2d-104          [-1, 4, 140, 210]             260
         outconv-105          [-1, 4, 140, 210]               0
================================================================
Total params: 17,107,208
Trainable params: 17,107,208
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 471.25
Params size (MB): 65.26
Estimated Total Size (MB): 536.85
----------------------------------------------------------------
Learning rate: 0.0004
2019-12-13 12:13:10.983690
Classifier weightL: 0.05
Using distance weighted Dice
Reading data
Loaded data
calculating weights
Training model
Dice: 8.2310670757215e-05
Loss: -0.15144005310958897
Err: [0.48793566 0.54021448 0.47184987 0.70509383]
Epoch 1 loss = -0.3878059293143451
-------------
Dice: 0.11953996527928426
Loss: -0.4368271757948435
Err: [0.46246649 0.44235925 0.52010724 0.44772118]
Epoch 2 loss = -0.7222882230207324
-------------
Dice: 0.2591905111311087
Loss: -0.5000044642604298
Err: [0.50134048 0.48927614 0.49329759 0.48525469]
Epoch 3 loss = -0.5222511321740846
-------------
Dice: 0.24895023222117957
Loss: -0.6715626352779996
Err: [0.48659517 0.49597855 0.49463807 0.48927614]
Epoch 4 loss = -0.6017249562963843
-------------
Dice: 0.27407868090786114
Loss: -0.44290470311776475
Err: [0.52546917 0.46246649 0.46380697 0.38739946]
