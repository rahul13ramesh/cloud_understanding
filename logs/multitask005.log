----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
          Dropout-45           [-1, 512, 8, 13]               0
           Conv2d-46           [-1, 256, 8, 13]       1,179,904
             ReLU-47           [-1, 256, 8, 13]               0
      BatchNorm2d-48           [-1, 256, 8, 13]             512
           Conv2d-49           [-1, 256, 8, 13]         590,080
             ReLU-50           [-1, 256, 8, 13]               0
      BatchNorm2d-51           [-1, 256, 8, 13]             512
           Conv2d-52           [-1, 256, 8, 13]         590,080
             ReLU-53           [-1, 256, 8, 13]               0
      BatchNorm2d-54           [-1, 256, 8, 13]             512
           Conv2d-55            [-1, 256, 4, 7]         590,080
             ReLU-56            [-1, 256, 4, 7]               0
      BatchNorm2d-57            [-1, 256, 4, 7]             512
             View-58                 [-1, 7168]               0
           Linear-59                    [-1, 4]          28,676
          Sigmoid-60                    [-1, 4]               0
  ConvTranspose2d-61          [-1, 512, 16, 26]       1,049,088
           Conv2d-62          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-63          [-1, 256, 17, 26]             512
             ReLU-64          [-1, 256, 17, 26]               0
           Conv2d-65          [-1, 256, 17, 26]         590,080
      BatchNorm2d-66          [-1, 256, 17, 26]             512
             ReLU-67          [-1, 256, 17, 26]               0
      double_conv-68          [-1, 256, 17, 26]               0
               up-69          [-1, 256, 17, 26]               0
  ConvTranspose2d-70          [-1, 256, 34, 52]         262,400
           Conv2d-71          [-1, 128, 35, 52]         589,952
      BatchNorm2d-72          [-1, 128, 35, 52]             256
             ReLU-73          [-1, 128, 35, 52]               0
           Conv2d-74          [-1, 128, 35, 52]         147,584
      BatchNorm2d-75          [-1, 128, 35, 52]             256
             ReLU-76          [-1, 128, 35, 52]               0
      double_conv-77          [-1, 128, 35, 52]               0
               up-78          [-1, 128, 35, 52]               0
  ConvTranspose2d-79         [-1, 128, 70, 104]          65,664
           Conv2d-80          [-1, 64, 70, 105]         147,520
      BatchNorm2d-81          [-1, 64, 70, 105]             128
             ReLU-82          [-1, 64, 70, 105]               0
           Conv2d-83          [-1, 64, 70, 105]          36,928
      BatchNorm2d-84          [-1, 64, 70, 105]             128
             ReLU-85          [-1, 64, 70, 105]               0
      double_conv-86          [-1, 64, 70, 105]               0
               up-87          [-1, 64, 70, 105]               0
  ConvTranspose2d-88         [-1, 64, 140, 210]          16,448
           Conv2d-89         [-1, 64, 140, 210]          73,792
      BatchNorm2d-90         [-1, 64, 140, 210]             128
             ReLU-91         [-1, 64, 140, 210]               0
           Conv2d-92         [-1, 64, 140, 210]          36,928
      BatchNorm2d-93         [-1, 64, 140, 210]             128
             ReLU-94         [-1, 64, 140, 210]               0
      double_conv-95         [-1, 64, 140, 210]               0
               up-96         [-1, 64, 140, 210]               0
           Conv2d-97          [-1, 4, 140, 210]             260
          outconv-98          [-1, 4, 140, 210]               0
================================================================
Total params: 17,769,992
Trainable params: 17,769,992
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 419.87
Params size (MB): 67.79
Estimated Total Size (MB): 488.00
----------------------------------------------------------------
Learning rate: 0.0004
2019-12-13 02:11:59.129101
Classifier weightL: 0.05
Using Dice loss
Reading data
Loaded data
Training model
Dice: 8.2310670757215e-05
Loss: 0.005274640409258021
Err: [0.51206434 0.54021448 0.52815013 0.70509383]
Epoch 1 loss = 0.2914968886816253
-------------
Dice: 0.06779380037159978
Loss: -0.16082562653684299
Err: [0.52949062 0.52546917 0.52815013 0.47855228]
Epoch 2 loss = -0.16515601890161632
-------------
Dice: 0.2304206366329588
Loss: -0.8310226019610458
Err: [0.5080429  0.51340483 0.4772118  0.41018767]
Epoch 3 loss = -0.2786452701066931
-------------
Dice: 0.053651651968922895
Loss: 2.2370577155872704
Err: [0.52680965 0.44772118 0.48525469 0.33512064]
Epoch 4 loss = -0.5673207859757046
-------------
Dice: 0.03911631779487291
Loss: -0.416641801948289
Err: [0.5080429  0.48525469 0.5        0.43163539]
Epoch 5 loss = -0.33996272654583054
-------------
Dice: 0.010911108418543062
Loss: -0.6190204695410197
Err: [0.49463807 0.47184987 0.4919571  0.39678284]
Epoch 6 loss = -0.4042777348371844
-------------
Dice: 0.009344382145177525
Loss: -0.6861428009697558
Err: [0.51206434 0.46246649 0.51206434 0.39276139]
Epoch 7 loss = -0.6118876857186357
-------------
Dice: 0.010011933268277198
Loss: -0.3552909868108621
Err: [0.4772118  0.4772118  0.48793566 0.3458445 ]
Epoch 8 loss = -2.4652771710200856
-------------
Dice: 0.05801837319808113
Loss: -0.9167284882883425
Err: [0.51474531 0.4919571  0.46112601 0.34450402]
Epoch 9 loss = 0.05443378517714639
-------------
Dice: 0.06907061242079016
Loss: -0.07108038921897292
Err: [0.48123324 0.47319035 0.46112601 0.31635389]
Epoch 10 loss = 0.05468517657990257
-------------
Dice: 0.06383511531385556
Loss: -0.07662213068658638
Err: [0.50268097 0.49329759 0.52010724 0.35656836]
Epoch 11 loss = 0.045669930983955664
-------------
Dice: 0.08072122009806777
Loss: -0.1717604814368349
Err: [0.51072386 0.41689008 0.49731903 0.31769437]
Epoch 12 loss = 0.03361478722964724
-------------
Dice: 0.05459075872707422
Loss: -0.14413791239165577
Err: [0.5080429  0.45308311 0.4919571  0.31367292]
Epoch 13 loss = 0.004851377168670297
-------------
Dice: 0.0666734142767553
Loss: -0.04548869860845733
Err: [0.4919571  0.46112601 0.50670241 0.30563003]
Epoch 14 loss = -0.03369887354783714
-------------
Dice: 0.06666547891310551
Loss: -0.16651131280297762
Err: [0.51742627 0.45844504 0.49731903 0.30428954]
Epoch 15 loss = -0.06685116910375655
-------------
Dice: 0.06603407971522922
Loss: -0.16948051697282113
Err: [0.52412869 0.50536193 0.48659517 0.40348525]
Epoch 16 loss = -0.11851251306012273
-------------
Dice: 0.06625830815924545
Loss: -0.1443191228453599
Err: [0.51608579 0.42895442 0.48257373 0.3230563 ]
Epoch 17 loss = -0.18090573622224232
-------------
Dice: 0.14670861160591905
Loss: -0.33339199453207863
Err: [0.5        0.48793566 0.52949062 0.38069705]
Epoch 18 loss = -0.2942427270921568
-------------
Dice: 0.14318506988208388
Loss: -0.16916351498511892
Err: [0.48793566 0.43297587 0.46380697 0.35522788]
Epoch 19 loss = -0.1521341437784334
-------------
Dice: 0.0803152308271332
Loss: -0.4018408264713184
Err: [0.51206434 0.45576408 0.48391421 0.3458445 ]
Epoch 20 loss = -0.2811905749794096
-------------
Dice: 0.12466002638674077
Loss: -0.6260180053811136
Err: [0.5227882  0.47050938 0.48525469 0.35120643]
Epoch 21 loss = -0.34466707046764594
-------------
Dice: 0.1208041087195388
Loss: -0.09848086329190052
Err: [0.49463807 0.47855228 0.51206434 0.38337802]
Epoch 22 loss = -0.49544135769829156
-------------
Dice: 0.010378358559042854
Loss: 0.9323749981799848
Err: [0.4919571  0.45040214 0.48257373 0.31769437]
Epoch 23 loss = -0.0554641801584512
-------------
Dice: 0.22060578628593147
Loss: -0.287051113058414
Err: [0.49865952 0.47453083 0.48659517 0.34986595]
Epoch 24 loss = -0.20101436843474707
-------------
Dice: 0.15747817325729724
Loss: -0.2670828937779926
Err: [0.5227882  0.46380697 0.47989276 0.34852547]
Epoch 25 loss = -0.16551866485737265
-------------
Dice: 0.1645848516777212
Loss: -0.4701357222583998
Err: [0.49865952 0.48257373 0.47855228 0.36327078]
Epoch 26 loss = -0.43696283106692135
-------------
Dice: 0.12934609171865993
Loss: -3.283301035428842
Err: [0.48391421 0.47587131 0.53485255 0.35120643]
Epoch 27 loss = -0.2695891583772997
-------------
Dice: 0.11876010423182692
Loss: -0.7782110580500153
Err: [0.50402145 0.49597855 0.51072386 0.37533512]
Epoch 28 loss = -0.26748046479187904
-------------
Dice: 0.2499274356752351
Loss: 0.17436809378898335
Err: [0.48659517 0.45442359 0.49865952 0.34182306]
Epoch 29 loss = -0.14250561859769126
-------------
Dice: 0.22827703245932673
Loss: -0.3740349700502952
Err: [0.51474531 0.49463807 0.47184987 0.36327078]
Epoch 30 loss = -0.4749387528343747
-------------
Dice: 0.19418771466850415
Loss: -0.23911660114848948
Err: [0.48123324 0.47050938 0.50134048 0.33512064]
Epoch 31 loss = -0.3191171209948758
-------------
Dice: 0.21765306212200006
Loss: -0.31305355470489127
Err: [0.49061662 0.46514745 0.48793566 0.35656836]
Epoch 32 loss = -0.27465621115018923
-------------
Dice: 0.26438427854968116
Loss: -0.4470621755264884
Err: [0.50134048 0.53753351 0.51072386 0.40482574]
Epoch 33 loss = -0.4659704423323274
-------------
Dice: 0.26479011857677753
Loss: -0.6010634513104792
Err: [0.50938338 0.47587131 0.48927614 0.39678284]
Epoch 34 loss = -0.5857197440322488
-------------
Dice: 0.1288448560286919
Loss: -0.11091473062503729
Err: [0.49731903 0.44235925 0.50402145 0.32841823]
Epoch 35 loss = -0.18445664796046912
-------------
Dice: 0.21282255104414308
Loss: -0.575729094177919
Err: [0.47453083 0.43297587 0.48123324 0.32841823]
Epoch 36 loss = -0.656627314525346
-------------
Dice: 0.22321065438201454
Loss: -0.7711193756951845
Err: [0.51206434 0.43297587 0.49061662 0.31367292]
Epoch 37 loss = -0.2101045133980612
-------------
Dice: 0.060786080353362096
Loss: 0.0248610795737399
Err: [0.49061662 0.49865952 0.51742627 0.33243968]
Epoch 38 loss = 0.08322040468454361
-------------
Dice: 0.12182080295509683
Loss: 0.1684571582448568
Err: [0.47855228 0.48123324 0.44906166 0.36461126]
Epoch 39 loss = -0.0906553495209664
-------------
Dice: 0.18378093787746547
Loss: -0.27758644244416714
Err: [0.50134048 0.45978552 0.50536193 0.35120643]
Epoch 40 loss = -0.26477771687321366
-------------
