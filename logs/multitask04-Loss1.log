----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
          Dropout-45           [-1, 512, 8, 13]               0
           Conv2d-46           [-1, 256, 8, 13]       1,179,904
             ReLU-47           [-1, 256, 8, 13]               0
      BatchNorm2d-48           [-1, 256, 8, 13]             512
           Conv2d-49           [-1, 256, 8, 13]         590,080
             ReLU-50           [-1, 256, 8, 13]               0
      BatchNorm2d-51           [-1, 256, 8, 13]             512
           Conv2d-52           [-1, 256, 8, 13]         590,080
             ReLU-53           [-1, 256, 8, 13]               0
      BatchNorm2d-54           [-1, 256, 8, 13]             512
           Conv2d-55            [-1, 256, 4, 7]         590,080
             ReLU-56            [-1, 256, 4, 7]               0
      BatchNorm2d-57            [-1, 256, 4, 7]             512
             View-58                 [-1, 7168]               0
           Linear-59                    [-1, 4]          28,676
          Sigmoid-60                    [-1, 4]               0
  ConvTranspose2d-61          [-1, 512, 16, 26]       1,049,088
           Conv2d-62          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-63          [-1, 256, 17, 26]             512
             ReLU-64          [-1, 256, 17, 26]               0
           Conv2d-65          [-1, 256, 17, 26]         590,080
      BatchNorm2d-66          [-1, 256, 17, 26]             512
             ReLU-67          [-1, 256, 17, 26]               0
      double_conv-68          [-1, 256, 17, 26]               0
               up-69          [-1, 256, 17, 26]               0
  ConvTranspose2d-70          [-1, 256, 34, 52]         262,400
           Conv2d-71          [-1, 128, 35, 52]         589,952
      BatchNorm2d-72          [-1, 128, 35, 52]             256
             ReLU-73          [-1, 128, 35, 52]               0
           Conv2d-74          [-1, 128, 35, 52]         147,584
      BatchNorm2d-75          [-1, 128, 35, 52]             256
             ReLU-76          [-1, 128, 35, 52]               0
      double_conv-77          [-1, 128, 35, 52]               0
               up-78          [-1, 128, 35, 52]               0
  ConvTranspose2d-79         [-1, 128, 70, 104]          65,664
           Conv2d-80          [-1, 64, 70, 105]         147,520
      BatchNorm2d-81          [-1, 64, 70, 105]             128
             ReLU-82          [-1, 64, 70, 105]               0
           Conv2d-83          [-1, 64, 70, 105]          36,928
      BatchNorm2d-84          [-1, 64, 70, 105]             128
             ReLU-85          [-1, 64, 70, 105]               0
      double_conv-86          [-1, 64, 70, 105]               0
               up-87          [-1, 64, 70, 105]               0
  ConvTranspose2d-88         [-1, 64, 140, 210]          16,448
           Conv2d-89         [-1, 64, 140, 210]          73,792
      BatchNorm2d-90         [-1, 64, 140, 210]             128
             ReLU-91         [-1, 64, 140, 210]               0
           Conv2d-92         [-1, 64, 140, 210]          36,928
      BatchNorm2d-93         [-1, 64, 140, 210]             128
             ReLU-94         [-1, 64, 140, 210]               0
      double_conv-95         [-1, 64, 140, 210]               0
               up-96         [-1, 64, 140, 210]               0
           Conv2d-97          [-1, 4, 140, 210]             260
          outconv-98          [-1, 4, 140, 210]               0
================================================================
Total params: 17,769,992
Trainable params: 17,769,992
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 419.87
Params size (MB): 67.79
Estimated Total Size (MB): 488.00
----------------------------------------------------------------
Learning rate: 0.0004
2019-12-13 02:11:21.678918
Classifier weightL: 0.4
Using distance weighted Dice
Reading data
Loaded data
calculating weights
Training model
Dice: 8.2310670757215e-05
Loss: -0.021064562759712968
Err: [0.51206434 0.54021448 0.47184987 0.70509383]
Epoch 1 loss = -0.07708245886179309
-------------
Dice: 0.07395145591232041
Loss: -0.7772703756870943
Err: [0.51206434 0.47319035 0.47453083 0.42895442]
Epoch 2 loss = -0.11252953556987147
-------------
Dice: 0.09369280408531941
Loss: -0.1995383962300193
Err: [0.52546917 0.50670241 0.4919571  0.39008043]
Epoch 3 loss = -0.3023858540536215
-------------
Dice: 0.11710929705802418
Loss: -0.33458969034235825
Err: [0.50536193 0.49463807 0.53887399 0.43565684]
Epoch 4 loss = -0.3298613958371182
-------------
Dice: 0.1702130477541745
Loss: -0.4169158127583968
Err: [0.51072386 0.51206434 0.49061662 0.34182306]
Epoch 5 loss = -0.17135906221655509
-------------
Dice: 0.18827578480903107
Loss: -0.4256014420445534
Err: [0.5        0.53619303 0.48257373 0.34986595]
Epoch 6 loss = -0.2591482350230217
-------------
Dice: 0.2275947706349728
Loss: -0.5149837777330029
Err: [0.48793566 0.50402145 0.50670241 0.32439678]
Epoch 7 loss = -0.3986657402540247
-------------
Dice: 0.26032416220207255
Loss: -0.618075859452031
Err: [0.5        0.49597855 0.52546917 0.29892761]
Epoch 8 loss = -0.5353858819045126
-------------
Dice: 0.2958924126309445
Loss: -0.6571047734959181
Err: [0.52010724 0.45844504 0.48525469 0.35924933]
Epoch 9 loss = -0.7929436449644466
-------------
Dice: 0.2608793693616537
Loss: -0.49690306887555563
Err: [0.54289544 0.49329759 0.5        0.33243968]
Epoch 10 loss = -0.7551670784316957
-------------
Dice: 0.18148784196526455
Loss: -0.5426499326488667
Err: [0.5        0.49463807 0.48927614 0.35522788]
Epoch 11 loss = -0.3164592783773939
-------------
Dice: 0.20326714455161835
Loss: -0.6545908155216867
Err: [0.48793566 0.46246649 0.52010724 0.33914209]
Epoch 12 loss = -0.4201279941946268
-------------
Dice: 0.22505288311002875
Loss: -0.6935401806013594
Err: [0.48659517 0.50134048 0.5        0.36058981]
Epoch 13 loss = -0.5014930440299213
-------------
Dice: 0.2376414061070642
Loss: -0.7362414683688472
Err: [0.49865952 0.47855228 0.46514745 0.36595174]
Epoch 14 loss = -0.6163600639191766
-------------
Dice: 0.2691166566238686
Loss: -0.8372356445614708
Err: [0.5        0.47587131 0.47989276 0.38739946]
Epoch 15 loss = -0.8433021879630784
-------------
Dice: 0.3084289233688063
Loss: -0.9933163663759408
Err: [0.48927614 0.49597855 0.45710456 0.36327078]
Epoch 16 loss = -1.3468023619179925
-------------
Dice: 0.30252224422352986
Loss: -0.9433090307071915
Err: [0.49597855 0.45308311 0.48793566 0.40884718]
Epoch 17 loss = -1.060870934339861
-------------
Dice: 0.2782706518951685
Loss: -0.8724071586585102
Err: [0.4919571  0.48123324 0.5308311  0.3538874 ]
Epoch 18 loss = -1.5281447382861126
-------------
Dice: 0.1413512813802192
Loss: -0.17524294301921478
Err: [0.50670241 0.46648794 0.49061662 0.33646113]
Epoch 19 loss = -0.1983136242441833
-------------
Dice: 0.18478613328915963
Loss: -0.35867836699513944
Err: [0.51340483 0.48123324 0.49731903 0.36193029]
Epoch 20 loss = -0.4132570646641155
-------------
Dice: 0.22307222198544055
Loss: -0.5697509704217635
Err: [0.5308311  0.52546917 0.51206434 0.36327078]
Epoch 21 loss = -0.6398925069160759
-------------
Dice: 0.23851981706754055
Loss: -0.7278567001126187
Err: [0.48793566 0.51742627 0.50268097 0.41420912]
Epoch 22 loss = -1.2174915893313785
-------------
Dice: 0.2834997858511258
Loss: -0.791629764746098
Err: [0.47453083 0.50268097 0.50536193 0.43967828]
Epoch 23 loss = -1.0197606213887533
-------------
Dice: 0.12245458741877549
Loss: -3.5999032936455935
Err: [0.49463807 0.45442359 0.46782842 0.34048257]
Epoch 24 loss = -0.7066667620340983
-------------
Dice: 0.13166452261494044
Loss: -0.19709109587551327
Err: [0.52144772 0.45308311 0.46648794 0.31903485]
Epoch 25 loss = 4.372680375240743
-------------
Dice: 0.11202210378987079
Loss: 0.6320277528470833
Err: [0.5        0.47050938 0.49731903 0.34450402]
Epoch 26 loss = -0.5956483352184295
-------------
Dice: 0.10801354873769452
Loss: 0.6940950883693504
Err: [0.49597855 0.4772118  0.52949062 0.31635389]
Epoch 27 loss = -0.337949144958208
-------------
Dice: 0.15088461319566174
Loss: 5.966877581023823
Err: [0.48793566 0.47050938 0.45442359 0.31635389]
Epoch 28 loss = -0.39166693716620404
-------------
Dice: 0.15623299046911712
Loss: 3.6752328259225755
Err: [0.47855228 0.4691689  0.50938338 0.3150134 ]
Epoch 29 loss = -0.4101383667097737
-------------
Dice: 0.1647998772594086
Loss: 1.6090108648531798
Err: [0.50268097 0.45442359 0.47855228 0.34718499]
Epoch 30 loss = -0.41500409684143963
-------------
Dice: 0.1614596622494137
Loss: -2.1964743275666017
Err: [0.50402145 0.5        0.49865952 0.33646113]
Epoch 31 loss = -0.4025264336106678
-------------
Dice: 0.1465072760976697
Loss: 2.165947590337179
Err: [0.53217158 0.47319035 0.45978552 0.35790885]
Epoch 32 loss = -0.40045177114817004
-------------
Dice: 0.15671614957391294
Loss: 1.2339384270922922
Err: [0.49865952 0.47855228 0.48659517 0.3310992 ]
Epoch 33 loss = -0.4131000476144254
-------------
Dice: 0.14774899632633523
Loss: 8.336718325097277
Err: [0.4772118  0.49061662 0.51206434 0.34450402]
Epoch 34 loss = -0.46564752934811016
-------------
Dice: 0.15152338705058438
Loss: 1.4598773706475916
Err: [0.51474531 0.47855228 0.50134048 0.3847185 ]
Epoch 35 loss = -0.45989237431747215
-------------
Dice: 0.1333454136899002
Loss: 11.740899882975512
Err: [0.52546917 0.4772118  0.53217158 0.32573727]
Epoch 36 loss = -0.49013054232423503
-------------
Dice: 0.13848678751828022
Loss: 1.6674739338548719
Err: [0.48391421 0.50134048 0.5        0.33780161]
Epoch 37 loss = -0.5935266560874879
-------------
Dice: 0.1340701968245136
Loss: -82.75161854465475
Err: [0.51608579 0.49865952 0.47319035 0.34182306]
Epoch 38 loss = -0.7347388418018818
-------------
Dice: 0.12178243196249398
Loss: 2.512905843540322
Err: [0.51474531 0.47050938 0.48525469 0.36327078]
Epoch 39 loss = -1.0370211073725173
-------------
Dice: 0.05458302939234119
Loss: 1.7131551343322815
Err: [0.48123324 0.48525469 0.50134048 0.34986595]
Epoch 40 loss = -1.2330032888613642
-------------
