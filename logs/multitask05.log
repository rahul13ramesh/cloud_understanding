----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
          Dropout-45           [-1, 512, 8, 13]               0
           Conv2d-46           [-1, 256, 8, 13]       1,179,904
             ReLU-47           [-1, 256, 8, 13]               0
      BatchNorm2d-48           [-1, 256, 8, 13]             512
           Conv2d-49           [-1, 256, 8, 13]         590,080
             ReLU-50           [-1, 256, 8, 13]               0
      BatchNorm2d-51           [-1, 256, 8, 13]             512
           Conv2d-52           [-1, 256, 8, 13]         590,080
             ReLU-53           [-1, 256, 8, 13]               0
      BatchNorm2d-54           [-1, 256, 8, 13]             512
           Conv2d-55            [-1, 256, 4, 7]         590,080
             ReLU-56            [-1, 256, 4, 7]               0
      BatchNorm2d-57            [-1, 256, 4, 7]             512
             View-58                 [-1, 7168]               0
           Linear-59                    [-1, 4]          28,676
          Sigmoid-60                    [-1, 4]               0
  ConvTranspose2d-61          [-1, 512, 16, 26]       1,049,088
           Conv2d-62          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-63          [-1, 256, 17, 26]             512
             ReLU-64          [-1, 256, 17, 26]               0
           Conv2d-65          [-1, 256, 17, 26]         590,080
      BatchNorm2d-66          [-1, 256, 17, 26]             512
             ReLU-67          [-1, 256, 17, 26]               0
      double_conv-68          [-1, 256, 17, 26]               0
               up-69          [-1, 256, 17, 26]               0
  ConvTranspose2d-70          [-1, 256, 34, 52]         262,400
           Conv2d-71          [-1, 128, 35, 52]         589,952
      BatchNorm2d-72          [-1, 128, 35, 52]             256
             ReLU-73          [-1, 128, 35, 52]               0
           Conv2d-74          [-1, 128, 35, 52]         147,584
      BatchNorm2d-75          [-1, 128, 35, 52]             256
             ReLU-76          [-1, 128, 35, 52]               0
      double_conv-77          [-1, 128, 35, 52]               0
               up-78          [-1, 128, 35, 52]               0
  ConvTranspose2d-79         [-1, 128, 70, 104]          65,664
           Conv2d-80          [-1, 64, 70, 105]         147,520
      BatchNorm2d-81          [-1, 64, 70, 105]             128
             ReLU-82          [-1, 64, 70, 105]               0
           Conv2d-83          [-1, 64, 70, 105]          36,928
      BatchNorm2d-84          [-1, 64, 70, 105]             128
             ReLU-85          [-1, 64, 70, 105]               0
      double_conv-86          [-1, 64, 70, 105]               0
               up-87          [-1, 64, 70, 105]               0
  ConvTranspose2d-88         [-1, 64, 140, 210]          16,448
           Conv2d-89         [-1, 64, 140, 210]          73,792
      BatchNorm2d-90         [-1, 64, 140, 210]             128
             ReLU-91         [-1, 64, 140, 210]               0
           Conv2d-92         [-1, 64, 140, 210]          36,928
      BatchNorm2d-93         [-1, 64, 140, 210]             128
             ReLU-94         [-1, 64, 140, 210]               0
      double_conv-95         [-1, 64, 140, 210]               0
               up-96         [-1, 64, 140, 210]               0
           Conv2d-97          [-1, 4, 140, 210]             260
          outconv-98          [-1, 4, 140, 210]               0
================================================================
Total params: 17,769,992
Trainable params: 17,769,992
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 419.87
Params size (MB): 67.79
Estimated Total Size (MB): 488.00
----------------------------------------------------------------
Learning rate: 0.0004
2019-12-13 02:11:41.200487
Classifier weightL: 0.5
Using Dice loss
Reading data
Loaded data
Training model
Dice: 8.2310670757215e-05
Loss: 0.0037842898482072563
Err: [0.48793566 0.45978552 0.47184987 0.70509383]
Epoch 1 loss = -0.14161447296850382
-------------
Dice: 0.022899013314038204
Loss: -0.344252371334801
Err: [0.45978552 0.43565684 0.4919571  0.3847185 ]
Epoch 2 loss = 0.311505557258303
-------------
Dice: 0.010626705035081955
Loss: 0.1252722421506293
Err: [0.51072386 0.48257373 0.51608579 0.30697051]
Epoch 3 loss = 0.22297989797467987
-------------
Dice: 0.022842769985684405
Loss: -4.410756434596898
Err: [0.5308311  0.4383378  0.47050938 0.43029491]
Epoch 4 loss = 0.12555019670166076
-------------
Dice: 0.03292280824525887
Loss: 0.04173835138035502
Err: [0.49329759 0.49463807 0.51876676 0.36461126]
Epoch 5 loss = -0.2490945686865598
-------------
Dice: 0.01915737915569633
Loss: 0.28280186753557124
Err: [0.48257373 0.47587131 0.4691689  0.35656836]
Epoch 6 loss = -0.808702725470066
-------------
Dice: 0.011126352076450817
Loss: -0.06359931183619214
Err: [0.48927614 0.4772118  0.51340483 0.41018767]
Epoch 7 loss = 0.21223982498670618
-------------
Dice: 0.16958321739697466
Loss: -1.0888018964463277
Err: [0.49597855 0.44369973 0.47184987 0.36595174]
Epoch 8 loss = -0.04637412439721326
-------------
Dice: 0.18594676656907047
Loss: -0.10281261015182795
Err: [0.50402145 0.44235925 0.49061662 0.38739946]
Epoch 9 loss = -0.10376696903569003
-------------
Dice: 0.18969650911609523
Loss: -0.29802605110700003
Err: [0.49597855 0.47453083 0.50536193 0.38069705]
Epoch 10 loss = -0.257802782614405
-------------
Dice: 0.0746063737760552
Loss: -0.13394522611813336
Err: [0.49463807 0.48659517 0.45710456 0.4075067 ]
Epoch 11 loss = -0.25153196685016155
-------------
Dice: 0.0561375614042457
Loss: -0.5317450671016841
Err: [0.49597855 0.4691689  0.52680965 0.35790885]
Epoch 12 loss = -0.08619759771352013
-------------
Dice: 0.1567095540486009
Loss: -0.2983601792478773
Err: [0.5080429  0.49865952 0.4772118  0.38873995]
Epoch 13 loss = -0.21427828210716446
-------------
Dice: 0.2131370514250888
Loss: -0.4522615457273553
Err: [0.49865952 0.47319035 0.50402145 0.33243968]
Epoch 14 loss = -0.18682612063363194
-------------
Dice: 0.005254366374651499
Loss: -0.053994342727416454
Err: [0.5080429  0.51206434 0.47184987 0.33646113]
Epoch 15 loss = 0.45568798488626877
-------------
Dice: 0.03850299612523193
Loss: -0.1857378788153482
Err: [0.51072386 0.46514745 0.46380697 0.31635389]
Epoch 16 loss = 0.2158710336467872
-------------
Dice: 0.09485681419097704
Loss: -0.13644572881760822
Err: [0.47184987 0.40214477 0.48391421 0.33378016]
Epoch 17 loss = -0.1517610962751011
-------------
Dice: 0.1508469880011698
Loss: -0.13256455574873785
Err: [0.47587131 0.44772118 0.4919571  0.33914209]
Epoch 18 loss = -0.09823665574193001
-------------
Dice: 0.06438972134687786
Loss: -0.18497403211259325
Err: [0.47319035 0.47989276 0.46782842 0.35924933]
Epoch 19 loss = 0.09145208751782775
-------------
Dice: 0.09438492007872028
Loss: -0.0611817126351408
Err: [0.50938338 0.47453083 0.50402145 0.32037534]
Epoch 20 loss = 0.12939376443314055
-------------
Dice: 0.08056811149127482
Loss: 0.43709737785571273
Err: [0.51876676 0.46514745 0.45844504 0.3310992 ]
Epoch 21 loss = -0.3166415424955388
-------------
Dice: 0.1445620772430462
Loss: -0.4534372081179559
Err: [0.47319035 0.40884718 0.48659517 0.31903485]
Epoch 22 loss = -0.3911226746812463
-------------
Dice: 0.04786755589250476
Loss: -5.063318358032949
Err: [0.5308311  0.48391421 0.47319035 0.34316354]
Epoch 23 loss = -0.5375349106608579
-------------
Dice: 0.15343154695374703
Loss: 1.7933489206182582
Err: [0.45844504 0.47050938 0.49597855 0.3150134 ]
Epoch 24 loss = -0.05419423041554789
-------------
Dice: 0.2497664046632782
Loss: -0.27319482466453765
Err: [0.52412869 0.45710456 0.49463807 0.33512064]
Epoch 25 loss = -0.2816920550105472
-------------
Dice: 0.265817027429262
Loss: 3.534141881742115
Err: [0.49061662 0.43431635 0.51072386 0.32573727]
Epoch 26 loss = 0.11542954398008684
-------------
Dice: 0.11677602725792738
Loss: -0.6773523512621086
Err: [0.48257373 0.47855228 0.50670241 0.3230563 ]
Epoch 27 loss = 0.0602429140266031
-------------
Dice: 0.16364411997360664
Loss: -0.1337916215898247
Err: [0.48391421 0.48123324 0.49329759 0.34718499]
Epoch 28 loss = -0.1701955099279682
-------------
Dice: 0.23796092496484045
Loss: -0.7103264618340547
Err: [0.46246649 0.45710456 0.46782842 0.3150134 ]
Epoch 29 loss = -0.5851726640667766
-------------
Dice: 0.11749049359615198
Loss: -0.1370556420821309
Err: [0.4919571  0.47050938 0.51072386 0.31233244]
Epoch 30 loss = -0.0672266651255389
-------------
Dice: 0.13988713953791826
Loss: -0.1751399021955418
Err: [0.51474531 0.43699732 0.51876676 0.30294906]
Epoch 31 loss = -0.13003757278124492
-------------
Dice: 0.16785567987884253
Loss: -0.2736521587708975
Err: [0.44906166 0.4463807  0.46380697 0.34316354]
Epoch 32 loss = -0.19043942985435328
-------------
Dice: 0.1948403469118031
Loss: -0.38715659001155195
Err: [0.48793566 0.47989276 0.46112601 0.30428954]
Epoch 33 loss = -0.3248257203338047
-------------
Dice: 0.10368454491379012
Loss: -0.9918823977776281
Err: [0.50134048 0.46648794 0.49865952 0.31099196]
Epoch 34 loss = -0.5038755153802534
-------------
Dice: 0.02720613825124603
Loss: 0.37672522291416255
Err: [0.5080429  0.47319035 0.50134048 0.41152815]
Epoch 35 loss = -0.4379716875590384
-------------
Dice: 0.18251087889114934
Loss: 6.841709451467621
Err: [0.48659517 0.47050938 0.52815013 0.34048257]
Epoch 36 loss = -0.47966344153198104
-------------
Dice: 0.02007783377178153
Loss: -0.0668473850806408
Err: [0.49731903 0.47050938 0.47184987 0.3538874 ]
Epoch 37 loss = 0.049210946339492995
-------------
Dice: 0.018237745169320984
Loss: -0.06554305598472115
Err: [0.50670241 0.47587131 0.46648794 0.35254692]
Epoch 38 loss = 0.042775354161858556
-------------
Dice: 0.021012772272230886
Loss: -0.12501854881491786
Err: [0.47050938 0.45978552 0.47050938 0.35522788]
Epoch 39 loss = 0.033744093303879105
-------------
Dice: 0.033553137867873055
Loss: -0.06719064412598907
Err: [0.52949062 0.49329759 0.4772118  0.37131367]
Epoch 40 loss = 0.017917001567160088
-------------
