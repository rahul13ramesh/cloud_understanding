| Wide-Resnet 28x4
/tools/miniconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 16, 140, 210]             448
       BatchNorm2d-2         [-1, 16, 140, 210]              32
            Conv2d-3         [-1, 64, 140, 210]           9,280
           Dropout-4         [-1, 64, 140, 210]               0
       BatchNorm2d-5         [-1, 64, 140, 210]             128
            Conv2d-6          [-1, 64, 70, 105]          36,928
            Conv2d-7          [-1, 64, 70, 105]           1,088
        wide_basic-8          [-1, 64, 70, 105]               0
       BatchNorm2d-9          [-1, 64, 70, 105]             128
           Conv2d-10          [-1, 64, 70, 105]          36,928
          Dropout-11          [-1, 64, 70, 105]               0
      BatchNorm2d-12          [-1, 64, 70, 105]             128
           Conv2d-13          [-1, 64, 70, 105]          36,928
       wide_basic-14          [-1, 64, 70, 105]               0
      BatchNorm2d-15          [-1, 64, 70, 105]             128
           Conv2d-16          [-1, 64, 70, 105]          36,928
          Dropout-17          [-1, 64, 70, 105]               0
      BatchNorm2d-18          [-1, 64, 70, 105]             128
           Conv2d-19          [-1, 64, 70, 105]          36,928
       wide_basic-20          [-1, 64, 70, 105]               0
      BatchNorm2d-21          [-1, 64, 70, 105]             128
           Conv2d-22          [-1, 64, 70, 105]          36,928
          Dropout-23          [-1, 64, 70, 105]               0
      BatchNorm2d-24          [-1, 64, 70, 105]             128
           Conv2d-25          [-1, 64, 70, 105]          36,928
       wide_basic-26          [-1, 64, 70, 105]               0
      BatchNorm2d-27          [-1, 64, 70, 105]             128
           Conv2d-28         [-1, 128, 70, 105]          73,856
          Dropout-29         [-1, 128, 70, 105]               0
      BatchNorm2d-30         [-1, 128, 70, 105]             256
           Conv2d-31          [-1, 128, 35, 53]         147,584
           Conv2d-32          [-1, 128, 35, 53]           8,320
       wide_basic-33          [-1, 128, 35, 53]               0
      BatchNorm2d-34          [-1, 128, 35, 53]             256
           Conv2d-35          [-1, 128, 35, 53]         147,584
          Dropout-36          [-1, 128, 35, 53]               0
      BatchNorm2d-37          [-1, 128, 35, 53]             256
           Conv2d-38          [-1, 128, 35, 53]         147,584
       wide_basic-39          [-1, 128, 35, 53]               0
      BatchNorm2d-40          [-1, 128, 35, 53]             256
           Conv2d-41          [-1, 128, 35, 53]         147,584
          Dropout-42          [-1, 128, 35, 53]               0
      BatchNorm2d-43          [-1, 128, 35, 53]             256
           Conv2d-44          [-1, 128, 35, 53]         147,584
       wide_basic-45          [-1, 128, 35, 53]               0
      BatchNorm2d-46          [-1, 128, 35, 53]             256
           Conv2d-47          [-1, 128, 35, 53]         147,584
          Dropout-48          [-1, 128, 35, 53]               0
      BatchNorm2d-49          [-1, 128, 35, 53]             256
           Conv2d-50          [-1, 128, 35, 53]         147,584
       wide_basic-51          [-1, 128, 35, 53]               0
      BatchNorm2d-52          [-1, 128, 35, 53]             256
           Conv2d-53          [-1, 256, 35, 53]         295,168
          Dropout-54          [-1, 256, 35, 53]               0
      BatchNorm2d-55          [-1, 256, 35, 53]             512
           Conv2d-56          [-1, 256, 18, 27]         590,080
           Conv2d-57          [-1, 256, 18, 27]          33,024
       wide_basic-58          [-1, 256, 18, 27]               0
      BatchNorm2d-59          [-1, 256, 18, 27]             512
           Conv2d-60          [-1, 256, 18, 27]         590,080
          Dropout-61          [-1, 256, 18, 27]               0
      BatchNorm2d-62          [-1, 256, 18, 27]             512
           Conv2d-63          [-1, 256, 18, 27]         590,080
       wide_basic-64          [-1, 256, 18, 27]               0
      BatchNorm2d-65          [-1, 256, 18, 27]             512
           Conv2d-66          [-1, 256, 18, 27]         590,080
          Dropout-67          [-1, 256, 18, 27]               0
      BatchNorm2d-68          [-1, 256, 18, 27]             512
           Conv2d-69          [-1, 256, 18, 27]         590,080
       wide_basic-70          [-1, 256, 18, 27]               0
      BatchNorm2d-71          [-1, 256, 18, 27]             512
           Conv2d-72          [-1, 256, 18, 27]         590,080
          Dropout-73          [-1, 256, 18, 27]               0
      BatchNorm2d-74          [-1, 256, 18, 27]             512
           Conv2d-75          [-1, 256, 18, 27]         590,080
       wide_basic-76          [-1, 256, 18, 27]               0
      BatchNorm2d-77          [-1, 256, 18, 27]             512
           Linear-78                    [-1, 4]           6,148
================================================================
Total params: 5,856,676
Trainable params: 5,856,676
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 222.34
Params size (MB): 22.34
Estimated Total Size (MB): 245.02
----------------------------------------------------------------
2019-12-13 11:55:50.937858
Reading data
Loaded data
----------
Test err: [0.4919571  0.42895442 0.47184987 0.6769437 ]
Test Loss: 0.6975040035497088
Epoch 1 loss = 0.6421521271516879
----------
Test err: [0.36058981 0.26675603 0.3458445  0.30563003]
Test Loss: 0.6124310874312756
Epoch 2 loss = 0.6007053390393654
----------
Test err: [0.39544236 0.33780161 0.42493298 0.26407507]
Test Loss: 0.7969796114748509
Epoch 3 loss = 0.5803110090394815
----------
Test err: [0.42895442 0.33646113 0.47453083 0.26273458]
Test Loss: 1.012101111797511
Epoch 4 loss = 0.5697731575494012
----------
Test err: [0.46514745 0.35656836 0.39946381 0.26273458]
Test Loss: 1.0266214419248696
Epoch 5 loss = 0.5624814033508301
----------
Test err: [0.41957105 0.31367292 0.44772118 0.25335121]
Test Loss: 0.9943655052237292
Epoch 6 loss = 0.5557891450822353
----------
Test err: [0.4075067  0.34182306 0.47050938 0.28016086]
Test Loss: 1.397943990511992
Epoch 7 loss = 0.5492611124242346
----------
Test err: [0.44906166 0.3538874  0.43163539 0.26675603]
Test Loss: 1.3612676844635638
Epoch 8 loss = 0.5443516622235378
----------
Test err: [0.47587131 0.32975871 0.36595174 0.26005362]
Test Loss: 1.1910104412528069
Epoch 9 loss = 0.5377116809164484
----------
Test err: [0.45442359 0.3150134  0.31769437 0.26273458]
Test Loss: 1.160806292179852
Epoch 10 loss = 0.5335716334482034
----------
Test err: [0.42895442 0.39410188 0.38069705 0.28150134]
Test Loss: 1.2067853371954795
Epoch 11 loss = 0.526716486774385
----------
Test err: [0.48793566 0.32439678 0.43163539 0.27882038]
Test Loss: 1.407429072603923
Epoch 12 loss = 0.5199011049047112
----------
Test err: [0.49329759 0.34316354 0.42627346 0.26675603]
Test Loss: 1.6796849639805398
Epoch 13 loss = 0.5141339970131715
----------
Test err: [0.49329759 0.30294906 0.48525469 0.32037534]
Test Loss: 1.488378945103567
Epoch 14 loss = 0.505572183628877
----------
Test err: [0.48257373 0.39142091 0.52680965 0.27211796]
Test Loss: 2.454500149379351
Epoch 15 loss = 0.4987631501381596
----------
Test err: [0.50536193 0.3458445  0.52815013 0.26809651]
Test Loss: 2.458204873872486
Epoch 16 loss = 0.4906668573742112
----------
Test err: [0.50670241 0.3847185  0.52815013 0.26273458]
Test Loss: 2.732803541467325
Epoch 17 loss = 0.48766660751154023
----------
Test err: [0.50268097 0.37265416 0.51608579 0.28016086]
Test Loss: 2.7483648108880385
Epoch 18 loss = 0.47666620907684165
----------
Test err: [0.50670241 0.31635389 0.51474531 0.2613941 ]
Test Loss: 2.58067270544724
Epoch 19 loss = 0.46302501930544776
----------
Test err: [0.50536193 0.33780161 0.52680965 0.29758713]
Test Loss: 2.2534743242532085
Epoch 20 loss = 0.4513022504746914
----------
Test err: [0.5080429  0.36327078 0.52815013 0.35790885]
Test Loss: 3.3100969193610315
Epoch 21 loss = 0.4387336532274882
----------
Test err: [0.50670241 0.37533512 0.52815013 0.4463807 ]
Test Loss: 3.227883461759925
Epoch 22 loss = 0.4250238794957598
----------
Test err: [0.50536193 0.4075067  0.5227882  0.42493298]
Test Loss: 3.072279102790779
Epoch 23 loss = 0.41231026583040753
----------
Test err: [0.50134048 0.35120643 0.52815013 0.29490617]
Test Loss: 2.612742150698212
Epoch 24 loss = 0.4003906243356566
----------
Test err: [0.5080429  0.30831099 0.52815013 0.48525469]
Test Loss: 4.727182278594335
Epoch 25 loss = 0.37984986203412213
----------
Test err: [0.50670241 0.27479893 0.52815013 0.2922252 ]
Test Loss: 3.829577709729938
