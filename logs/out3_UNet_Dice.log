----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
  ConvTranspose2d-45          [-1, 512, 16, 26]       1,049,088
           Conv2d-46          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-47          [-1, 256, 17, 26]             512
             ReLU-48          [-1, 256, 17, 26]               0
           Conv2d-49          [-1, 256, 17, 26]         590,080
      BatchNorm2d-50          [-1, 256, 17, 26]             512
             ReLU-51          [-1, 256, 17, 26]               0
      double_conv-52          [-1, 256, 17, 26]               0
               up-53          [-1, 256, 17, 26]               0
  ConvTranspose2d-54          [-1, 256, 34, 52]         262,400
           Conv2d-55          [-1, 128, 35, 52]         589,952
      BatchNorm2d-56          [-1, 128, 35, 52]             256
             ReLU-57          [-1, 128, 35, 52]               0
           Conv2d-58          [-1, 128, 35, 52]         147,584
      BatchNorm2d-59          [-1, 128, 35, 52]             256
             ReLU-60          [-1, 128, 35, 52]               0
      double_conv-61          [-1, 128, 35, 52]               0
               up-62          [-1, 128, 35, 52]               0
  ConvTranspose2d-63         [-1, 128, 70, 104]          65,664
           Conv2d-64          [-1, 64, 70, 105]         147,520
      BatchNorm2d-65          [-1, 64, 70, 105]             128
             ReLU-66          [-1, 64, 70, 105]               0
           Conv2d-67          [-1, 64, 70, 105]          36,928
      BatchNorm2d-68          [-1, 64, 70, 105]             128
             ReLU-69          [-1, 64, 70, 105]               0
      double_conv-70          [-1, 64, 70, 105]               0
               up-71          [-1, 64, 70, 105]               0
  ConvTranspose2d-72         [-1, 64, 140, 210]          16,448
           Conv2d-73         [-1, 64, 140, 210]          73,792
      BatchNorm2d-74         [-1, 64, 140, 210]             128
             ReLU-75         [-1, 64, 140, 210]               0
           Conv2d-76         [-1, 64, 140, 210]          36,928
      BatchNorm2d-77         [-1, 64, 140, 210]             128
             ReLU-78         [-1, 64, 140, 210]               0
      double_conv-79         [-1, 64, 140, 210]               0
               up-80         [-1, 64, 140, 210]               0
           Conv2d-81          [-1, 4, 140, 210]             260
          outconv-82          [-1, 4, 140, 210]               0
================================================================
Total params: 14,789,124
Trainable params: 14,789,124
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 417.42
Params size (MB): 56.42
Estimated Total Size (MB): 474.17
----------------------------------------------------------------
Learning rate: 0.005
Augmentation: 1
Using Dice loss
Reading data
Loaded data
Training model
Epoch 1 loss = -0.4129008025800188
Epoch 2 loss = -0.4376191722229123
Epoch 3 loss = -0.4455271779000759
Epoch 4 loss = -0.45269381971408923
Epoch 5 loss = -0.4569905023773511
Epoch 6 loss = -0.473585908425351
Epoch 7 loss = -0.4926463501776258
Epoch 8 loss = -0.49768448798606796
Epoch 9 loss = -0.503342900549372
Epoch 10 loss = -0.5055481002603968
Epoch 11 loss = -0.5101454090202848
Epoch 12 loss = -0.5142697407181064
Epoch 13 loss = -0.5193167896320422
Epoch 14 loss = -0.5242424179737767
Epoch 15 loss = -0.5267268586158752
Epoch 16 loss = -0.5310252388566732
Epoch 17 loss = -0.5337024298993249
Epoch 18 loss = -0.5374781169742345
Epoch 19 loss = -0.5398796700624128
Epoch 20 loss = -0.5426442593087752
Epoch 21 loss = -0.5456341077387333
Epoch 22 loss = -0.5481031097347537
Epoch 23 loss = -0.5489438417802255
Epoch 24 loss = -0.5515349772448341
Epoch 25 loss = -0.5534474776064356
Epoch 26 loss = -0.554993856276075
Epoch 27 loss = -0.5564789205044508
Epoch 28 loss = -0.5599301870415608
Epoch 29 loss = -0.5601350534210603
Epoch 30 loss = -0.5645472632100185
Epoch 31 loss = -0.5648954227070013
Epoch 32 loss = -0.5670280402898789
Epoch 33 loss = -0.5692952777196963
Epoch 34 loss = -0.5712255907058715
Epoch 35 loss = -0.5720525888105233
Epoch 36 loss = -0.5752641671026747
Epoch 37 loss = -0.576655526459217
Epoch 38 loss = -0.5768281516060233
Epoch 39 loss = -0.5772883351395528
Epoch 40 loss = -0.5798375948394339
Epoch 41 loss = -0.5821554481362303
Epoch 42 loss = -0.5840561482682824
Epoch 43 loss = -0.5842372010524074
Epoch 44 loss = -0.5858243144924442
Epoch 45 loss = -0.5882136067623893
Epoch 46 loss = -0.588849129329125
Epoch 47 loss = -0.5910161447028319
Epoch 48 loss = -0.5925596973796685
Epoch 49 loss = -0.5938587364678581
Epoch 50 loss = -0.5952242862433195
Epoch 51 loss = -0.5973370615641276
Epoch 52 loss = -0.5983564152071873
Epoch 53 loss = -0.6005480663478374
Epoch 54 loss = -0.6018064830079675
Epoch 55 loss = -0.6041302929446101
Epoch 56 loss = -0.6040053877234459
Epoch 57 loss = -0.606781685873866
Epoch 58 loss = -0.6082403248051802
Epoch 59 loss = -0.6084411416947841
Epoch 60 loss = -0.6075315384815136
Epoch 61 loss = -0.6104014245172341
Epoch 62 loss = -0.6121762701620658
Epoch 63 loss = -0.6143377626066406
Epoch 64 loss = -0.616953768543899
Epoch 65 loss = -0.6186447500437499
Epoch 66 loss = -0.6210201318065326
Epoch 67 loss = -0.6185912051796914
Epoch 68 loss = -0.6228654787565271
Epoch 69 loss = -0.6255159024397532
Epoch 70 loss = -0.6246954367185632
Epoch 71 loss = -0.6289642884209752
Epoch 72 loss = -0.6290314068396886
Epoch 73 loss = -0.6320525988563895
Epoch 74 loss = -0.6325376878678799
Epoch 75 loss = -0.6348039314895868
Epoch 76 loss = -0.6355645278096199
Epoch 77 loss = -0.6392453641196092
Epoch 78 loss = -0.6370367319757739
Epoch 79 loss = -0.6368277627850573
Epoch 80 loss = -0.641820072333018
Dice: 0.5360669951474442
Loss: -0.5360509506285817
