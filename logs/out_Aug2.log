----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
  ConvTranspose2d-45          [-1, 512, 16, 26]       1,049,088
           Conv2d-46          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-47          [-1, 256, 17, 26]             512
             ReLU-48          [-1, 256, 17, 26]               0
           Conv2d-49          [-1, 256, 17, 26]         590,080
      BatchNorm2d-50          [-1, 256, 17, 26]             512
             ReLU-51          [-1, 256, 17, 26]               0
      double_conv-52          [-1, 256, 17, 26]               0
               up-53          [-1, 256, 17, 26]               0
  ConvTranspose2d-54          [-1, 256, 34, 52]         262,400
           Conv2d-55          [-1, 128, 35, 52]         589,952
      BatchNorm2d-56          [-1, 128, 35, 52]             256
             ReLU-57          [-1, 128, 35, 52]               0
           Conv2d-58          [-1, 128, 35, 52]         147,584
      BatchNorm2d-59          [-1, 128, 35, 52]             256
             ReLU-60          [-1, 128, 35, 52]               0
      double_conv-61          [-1, 128, 35, 52]               0
               up-62          [-1, 128, 35, 52]               0
  ConvTranspose2d-63         [-1, 128, 70, 104]          65,664
           Conv2d-64          [-1, 64, 70, 105]         147,520
      BatchNorm2d-65          [-1, 64, 70, 105]             128
             ReLU-66          [-1, 64, 70, 105]               0
           Conv2d-67          [-1, 64, 70, 105]          36,928
      BatchNorm2d-68          [-1, 64, 70, 105]             128
             ReLU-69          [-1, 64, 70, 105]               0
      double_conv-70          [-1, 64, 70, 105]               0
               up-71          [-1, 64, 70, 105]               0
  ConvTranspose2d-72         [-1, 64, 140, 210]          16,448
           Conv2d-73         [-1, 64, 140, 210]          73,792
      BatchNorm2d-74         [-1, 64, 140, 210]             128
             ReLU-75         [-1, 64, 140, 210]               0
           Conv2d-76         [-1, 64, 140, 210]          36,928
      BatchNorm2d-77         [-1, 64, 140, 210]             128
             ReLU-78         [-1, 64, 140, 210]               0
      double_conv-79         [-1, 64, 140, 210]               0
               up-80         [-1, 64, 140, 210]               0
           Conv2d-81          [-1, 4, 140, 210]             260
          outconv-82          [-1, 4, 140, 210]               0
================================================================
Total params: 14,789,124
Trainable params: 14,789,124
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 417.42
Params size (MB): 56.42
Estimated Total Size (MB): 474.17
----------------------------------------------------------------
Learning rate: 0.0004
Augmentation: 2
Using distance weighted Dice
Reading data
Loaded data
calculating weights
Training model
Dice: 0.22811836575454422
Loss: -0.6718144571813118
Epoch 1 loss = -0.6965052000060677
-------------
Dice: 0.43144779745994594
Loss: -0.8273497866828684
Epoch 2 loss = -0.7570682902385791
-------------
Dice: 0.43222564861442564
Loss: -0.7878858624395244
Epoch 3 loss = -0.7855152193456888
-------------
Dice: 0.4488136804274823
Loss: -0.8177608599758803
Epoch 4 loss = -0.7927285190299154
-------------
Dice: 0.45443594236914875
Loss: -0.7968113180492006
Epoch 5 loss = -0.7994455235699812
-------------
Dice: 0.4612363026645933
Loss: -0.8291642611328077
Epoch 6 loss = -0.8209486248095831
-------------
Dice: 0.4595683362370434
Loss: -0.6805430732020179
Epoch 7 loss = -0.8354400379086534
-------------
Dice: 0.49757825496903313
Loss: -0.7592200066237175
Epoch 8 loss = -0.8369996811077484
-------------
Dice: 0.49399768666507554
Loss: -0.7791680592603438
Epoch 9 loss = -0.8433968389655153
-------------
Dice: 0.4943785739665383
Loss: -0.8191143234753155
Epoch 10 loss = -0.8544612454498808
-------------
Dice: 0.5054210031955116
Loss: -0.8001839965593988
Epoch 11 loss = -0.8665814222892125
-------------
Dice: 0.5109172383675452
Loss: -0.7794841559283102
Epoch 12 loss = -0.8583963314474871
-------------
Dice: 0.5099368981729876
Loss: -0.7910052835330954
Epoch 13 loss = -0.8626593317960699
-------------
Dice: 0.49848962336201436
Loss: -0.7938873951641522
Epoch 14 loss = -0.8717547643557191
-------------
Dice: 0.526145610565447
Loss: -0.7932272179130806
Epoch 15 loss = -0.8693540776645143
-------------
Dice: 0.5115318310206927
Loss: -0.768921728921193
Epoch 16 loss = -0.8812549252063036
-------------
Dice: 0.5225543017600137
Loss: -0.808139660099962
Epoch 17 loss = -0.8814321520676216
-------------
Dice: 0.527487721130053
Loss: -0.79368421663197
Epoch 18 loss = -0.881630845318238
-------------
Dice: 0.5161921406495642
Loss: -0.7784579483514977
Epoch 19 loss = -0.8934761416167021
-------------
Dice: 0.5223404163045592
Loss: -0.793566486326421
Epoch 20 loss = -0.8852462928990523
-------------
Dice: 0.5262983970964465
Loss: -0.77573818306604
Epoch 21 loss = -0.8888394247757242
-------------
Dice: 0.5160899232887596
Loss: -0.7543935967238925
Epoch 22 loss = -0.9031378512084484
-------------
Dice: 0.5305369383856245
Loss: -0.8234518340392661
Epoch 23 loss = -0.8942525485282143
-------------
Dice: 0.5331661244206947
Loss: -0.8132955531092211
Epoch 24 loss = -0.9105845963706573
-------------
Dice: 0.5270287154284677
Loss: -0.7956865296165251
Epoch 25 loss = -0.9037250429267685
-------------
Dice: 0.5342203797917983
Loss: -0.801173005452785
Epoch 26 loss = -0.9141293755546213
-------------
Dice: 0.526157671011243
Loss: -0.7608033732427362
Epoch 27 loss = -0.9105113641421
-------------
Dice: 0.532136183123679
Loss: -0.8300588238827727
Epoch 28 loss = -0.9096389420578878
-------------
Dice: 0.5265201778016432
Loss: -0.8170141629357509
Epoch 29 loss = -0.904838634673506
-------------
Dice: 0.5360600247864057
Loss: -0.8177386692730252
Epoch 30 loss = -0.9150788009787599
-------------
Dice: 0.5423058084949273
Loss: -0.7791949493710895
Epoch 31 loss = -0.9049279186626276
-------------
Dice: 0.5242503898428176
Loss: -0.7856623783936567
Epoch 32 loss = -0.9198437836766243
-------------
Dice: 0.5276999007625911
Loss: -0.7851055289446223
Epoch 33 loss = -0.9169908929616213
-------------
Dice: 0.5203323837685612
Loss: -0.8275388856449348
Epoch 34 loss = -0.9115069555863738
-------------
Dice: 0.5362461359577784
Loss: -0.8140076540275182
Epoch 35 loss = -0.9362333199878534
-------------
Dice: 0.5348729612799334
Loss: -0.8021147434170446
Epoch 36 loss = -0.920367190502584
-------------
Dice: 0.5258016152661694
Loss: -0.8085826048804713
Epoch 37 loss = -0.9190326522787412
-------------
Dice: 0.5489348747965875
Loss: -0.7998038580048423
Epoch 38 loss = -0.9107776214120289
-------------
Dice: 0.5293693797204514
Loss: -0.7830573006240009
Epoch 39 loss = -0.9200901233653227
-------------
Dice: 0.5406981095691485
Loss: -0.768127334472842
Epoch 40 loss = -0.9297937838236491
-------------
