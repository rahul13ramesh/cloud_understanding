----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
          Dropout-45           [-1, 512, 8, 13]               0
           Conv2d-46           [-1, 256, 8, 13]       1,179,904
             ReLU-47           [-1, 256, 8, 13]               0
      BatchNorm2d-48           [-1, 256, 8, 13]             512
           Conv2d-49           [-1, 256, 8, 13]         590,080
             ReLU-50           [-1, 256, 8, 13]               0
      BatchNorm2d-51           [-1, 256, 8, 13]             512
           Conv2d-52           [-1, 256, 8, 13]         590,080
             ReLU-53           [-1, 256, 8, 13]               0
      BatchNorm2d-54           [-1, 256, 8, 13]             512
           Conv2d-55            [-1, 256, 4, 7]         590,080
             ReLU-56            [-1, 256, 4, 7]               0
      BatchNorm2d-57            [-1, 256, 4, 7]             512
             View-58                 [-1, 7168]               0
           Linear-59                    [-1, 4]          28,676
          Sigmoid-60                    [-1, 4]               0
  ConvTranspose2d-61          [-1, 512, 16, 26]       1,049,088
           Conv2d-62          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-63          [-1, 256, 17, 26]             512
             ReLU-64          [-1, 256, 17, 26]               0
           Conv2d-65          [-1, 256, 17, 26]         590,080
      BatchNorm2d-66          [-1, 256, 17, 26]             512
             ReLU-67          [-1, 256, 17, 26]               0
      double_conv-68          [-1, 256, 17, 26]               0
               up-69          [-1, 256, 17, 26]               0
  ConvTranspose2d-70          [-1, 256, 34, 52]         262,400
           Conv2d-71          [-1, 128, 35, 52]         589,952
      BatchNorm2d-72          [-1, 128, 35, 52]             256
             ReLU-73          [-1, 128, 35, 52]               0
           Conv2d-74          [-1, 128, 35, 52]         147,584
      BatchNorm2d-75          [-1, 128, 35, 52]             256
             ReLU-76          [-1, 128, 35, 52]               0
      double_conv-77          [-1, 128, 35, 52]               0
               up-78          [-1, 128, 35, 52]               0
  ConvTranspose2d-79         [-1, 128, 70, 104]          65,664
           Conv2d-80          [-1, 64, 70, 105]         147,520
      BatchNorm2d-81          [-1, 64, 70, 105]             128
             ReLU-82          [-1, 64, 70, 105]               0
           Conv2d-83          [-1, 64, 70, 105]          36,928
      BatchNorm2d-84          [-1, 64, 70, 105]             128
             ReLU-85          [-1, 64, 70, 105]               0
      double_conv-86          [-1, 64, 70, 105]               0
               up-87          [-1, 64, 70, 105]               0
  ConvTranspose2d-88         [-1, 64, 140, 210]          16,448
           Conv2d-89         [-1, 64, 140, 210]          73,792
      BatchNorm2d-90         [-1, 64, 140, 210]             128
             ReLU-91         [-1, 64, 140, 210]               0
           Conv2d-92         [-1, 64, 140, 210]          36,928
      BatchNorm2d-93         [-1, 64, 140, 210]             128
             ReLU-94         [-1, 64, 140, 210]               0
      double_conv-95         [-1, 64, 140, 210]               0
               up-96         [-1, 64, 140, 210]               0
           Conv2d-97          [-1, 4, 140, 210]             260
          outconv-98          [-1, 4, 140, 210]               0
================================================================
Total params: 17,769,992
Trainable params: 17,769,992
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 419.87
Params size (MB): 67.79
Estimated Total Size (MB): 488.00
----------------------------------------------------------------
Learning rate: 0.0004
2019-12-13 02:12:24.806225
Classifier weightL: 0.05
Using distance weighted Dice
Reading data
Loaded data
calculating weights
Training model
Dice: 8.2310670757215e-05
Loss: -0.018493180958327367
Err: [0.48793566 0.45978552 0.52815013 0.29490617]
Epoch 1 loss = -0.5659132614483436
-------------
Dice: 0.20085217543163572
Loss: 0.16908540116528165
Err: [0.5080429  0.45174263 0.48257373 0.33646113]
Epoch 2 loss = -0.5700152062997222
-------------
Dice: 0.1893037490032071
Loss: 0.7852310065545071
Err: [0.51474531 0.4772118  0.48525469 0.32841823]
Epoch 3 loss = -0.7559220598389705
-------------
Dice: 0.19841141662618356
Loss: -0.04483325602583482
Err: [0.48793566 0.50670241 0.52546917 0.50134048]
Epoch 4 loss = -0.4000474020776649
-------------
Dice: 0.20320293431148412
Loss: -0.16685122274902922
Err: [0.51206434 0.45308311 0.4691689  0.36729223]
Epoch 5 loss = -0.6879835384928932
-------------
Dice: 0.020144030269398986
Loss: 0.7173218697946735
Err: [0.48793566 0.47319035 0.51340483 0.37131367]
Epoch 6 loss = -0.43427465847382946
-------------
Dice: 0.12141962815787137
Loss: -0.13996149922780232
Err: [0.48525469 0.49865952 0.50938338 0.3310992 ]
Epoch 7 loss = -0.15613350876607002
-------------
Dice: 0.15103317161576452
Loss: -0.23270285951923445
Err: [0.51608579 0.46514745 0.47989276 0.2922252 ]
Epoch 8 loss = -0.2899806108636161
-------------
Dice: 0.2014844603681713
Loss: -0.31165552705394645
Err: [0.48927614 0.45040214 0.48659517 0.34852547]
Epoch 9 loss = -0.49597141379490495
-------------
Dice: 0.25181841677600375
Loss: -0.4304283339709378
Err: [0.51474531 0.48659517 0.54021448 0.38069705]
Epoch 10 loss = -0.5843078617627422
-------------
Dice: 0.003715584263905263
Loss: 1.6173754789018828
Err: [0.5080429  0.49865952 0.47050938 0.34048257]
Epoch 11 loss = -0.6781814445803562
-------------
Dice: 0.003849854358514209
Loss: 6.575247930742965
Err: [0.50268097 0.46782842 0.47989276 0.32975871]
Epoch 12 loss = -0.7728676779009401
-------------
Dice: 0.0065760663825868
Loss: 1.7547117522967626
Err: [0.5227882  0.43163539 0.50670241 0.35656836]
Epoch 13 loss = -1.3716809026027719
-------------
Dice: 0.0678000008090542
Loss: 0.11467302015635555
Err: [0.53619303 0.45040214 0.46380697 0.34316354]
Epoch 14 loss = 0.03644701193707685
-------------
Dice: 0.07209050690600599
Loss: 0.2866883812081195
Err: [0.48525469 0.48927614 0.51742627 0.33512064]
Epoch 15 loss = 0.010011536423116923
-------------
Dice: 0.0698960272199623
Loss: 0.2595689547968056
Err: [0.50938338 0.4691689  0.47184987 0.36863271]
Epoch 16 loss = -0.04530489915361007
-------------
Dice: 0.08355509725101971
Loss: -0.3660877573993007
Err: [0.49597855 0.48391421 0.50938338 0.28954424]
Epoch 17 loss = -0.12303891197778284
-------------
Dice: 0.1537363522736061
Loss: -0.39596130524530393
Err: [0.49865952 0.44369973 0.5        0.36058981]
Epoch 18 loss = -0.296750868751357
-------------
Dice: 0.2071587318364456
Loss: -0.32711377777171696
Err: [0.50938338 0.48123324 0.50402145 0.32975871]
Epoch 19 loss = -0.6807981146685779
-------------
Dice: 0.073813886393657
Loss: 0.8854138346004422
Err: [0.48793566 0.46112601 0.47050938 0.33914209]
Epoch 20 loss = -0.7311106980095307
-------------
Dice: 0.060433422389425576
Loss: 2.546009433682978
Err: [0.50134048 0.48927614 0.50402145 0.38873995]
Epoch 21 loss = -1.1798811954818667
-------------
Dice: 0.021984705744991666
Loss: -7.0697166049058975
Err: [0.51608579 0.4463807  0.52949062 0.34048257]
Epoch 22 loss = -0.9152391989218692
-------------
Dice: 0.014668629090718071
Loss: 6.806139844314922
Err: [0.52412869 0.47989276 0.52010724 0.41286863]
Epoch 23 loss = -0.9325598247721791
-------------
Dice: 0.020624249946897606
Loss: 4.9063944803406025
Err: [0.47855228 0.48391421 0.46648794 0.39276139]
Epoch 24 loss = 6.292374044380461
-------------
Dice: 0.08310810518151346
Loss: 0.8201750326937879
Err: [0.5        0.46246649 0.5080429  0.39410188]
Epoch 25 loss = 1.0628513304982334
-------------
Dice: 0.08475598632564368
Loss: 0.7392339036344605
Err: [0.54289544 0.46112601 0.5        0.40616622]
Epoch 26 loss = 0.6395437079202384
-------------
Dice: 0.08984950104055056
Loss: 0.5013180027977874
Err: [0.48927614 0.49731903 0.49463807 0.41018767]
Epoch 27 loss = 0.620040966446201
-------------
Dice: 0.0935317065996558
Loss: 0.4772885793006755
Err: [0.53485255 0.4772118  0.50670241 0.37399464]
Epoch 28 loss = 0.5717649536797156
-------------
Dice: 0.09090828166246681
Loss: 0.46678655355748655
Err: [0.47989276 0.48525469 0.48123324 0.41152815]
Epoch 29 loss = 0.45919624728771546
-------------
Dice: 0.09535677956795655
Loss: 0.37744762539728133
Err: [0.46246649 0.48793566 0.48793566 0.39812332]
Epoch 30 loss = 0.31987376815018553
-------------
Dice: 0.0967332008233015
Loss: 0.2739253866231261
Err: [0.46648794 0.5        0.49865952 0.40348525]
Epoch 31 loss = 0.18283460594092807
-------------
Dice: 0.09618735989424275
Loss: 0.13391947985255975
Err: [0.49731903 0.50134048 0.50268097 0.36997319]
Epoch 32 loss = 0.022347160655384262
-------------
Dice: 0.10650833109204216
Loss: 0.010468571366098224
Err: [0.48793566 0.48257373 0.49865952 0.36595174]
Epoch 33 loss = -0.18086887763192255
-------------
Dice: 0.14156257310415662
Loss: -0.11087362451615229
Err: [0.51474531 0.46246649 0.51206434 0.33378016]
Epoch 34 loss = -0.6067188817976663
-------------
Dice: 0.15086325655636074
Loss: 0.3635479857039172
Err: [0.50134048 0.48525469 0.50402145 0.42359249]
Epoch 35 loss = -0.31397103808199367
-------------
Dice: 0.23282154732834018
Loss: -0.045169277004298354
Err: [0.46514745 0.52546917 0.51206434 0.3766756 ]
Epoch 36 loss = -0.5052283118323734
-------------
Dice: 0.23774183134506924
Loss: -0.29595119889198873
Err: [0.51608579 0.49731903 0.51340483 0.37399464]
Epoch 37 loss = -0.7489645127393305
-------------
Dice: 0.2669837890776324
Loss: -0.33657467130062246
Err: [0.45844504 0.46112601 0.50134048 0.36058981]
Epoch 38 loss = -1.5014361736675104
-------------
Dice: 0.053265478671544554
Loss: 2.4415257569039164
Err: [0.5        0.5080429  0.4919571  0.37935657]
Epoch 39 loss = -0.18848358892214795
-------------
Dice: 0.0318922679393178
Loss: 0.11804021306421911
Err: [0.50670241 0.45710456 0.49731903 0.38069705]
Epoch 40 loss = 0.3707695233480384
-------------
