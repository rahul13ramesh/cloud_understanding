----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
  ConvTranspose2d-45          [-1, 512, 16, 26]       1,049,088
           Conv2d-46          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-47          [-1, 256, 17, 26]             512
             ReLU-48          [-1, 256, 17, 26]               0
           Conv2d-49          [-1, 256, 17, 26]         590,080
      BatchNorm2d-50          [-1, 256, 17, 26]             512
             ReLU-51          [-1, 256, 17, 26]               0
      double_conv-52          [-1, 256, 17, 26]               0
               up-53          [-1, 256, 17, 26]               0
  ConvTranspose2d-54          [-1, 256, 34, 52]         262,400
           Conv2d-55          [-1, 128, 35, 52]         589,952
      BatchNorm2d-56          [-1, 128, 35, 52]             256
             ReLU-57          [-1, 128, 35, 52]               0
           Conv2d-58          [-1, 128, 35, 52]         147,584
      BatchNorm2d-59          [-1, 128, 35, 52]             256
             ReLU-60          [-1, 128, 35, 52]               0
      double_conv-61          [-1, 128, 35, 52]               0
               up-62          [-1, 128, 35, 52]               0
  ConvTranspose2d-63         [-1, 128, 70, 104]          65,664
           Conv2d-64          [-1, 64, 70, 105]         147,520
      BatchNorm2d-65          [-1, 64, 70, 105]             128
             ReLU-66          [-1, 64, 70, 105]               0
           Conv2d-67          [-1, 64, 70, 105]          36,928
      BatchNorm2d-68          [-1, 64, 70, 105]             128
             ReLU-69          [-1, 64, 70, 105]               0
      double_conv-70          [-1, 64, 70, 105]               0
               up-71          [-1, 64, 70, 105]               0
  ConvTranspose2d-72         [-1, 64, 140, 210]          16,448
           Conv2d-73         [-1, 64, 140, 210]          73,792
      BatchNorm2d-74         [-1, 64, 140, 210]             128
             ReLU-75         [-1, 64, 140, 210]               0
           Conv2d-76         [-1, 64, 140, 210]          36,928
      BatchNorm2d-77         [-1, 64, 140, 210]             128
             ReLU-78         [-1, 64, 140, 210]               0
      double_conv-79         [-1, 64, 140, 210]               0
               up-80         [-1, 64, 140, 210]               0
           Conv2d-81          [-1, 4, 140, 210]             260
          outconv-82          [-1, 4, 140, 210]               0
================================================================
Total params: 14,789,124
Trainable params: 14,789,124
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 417.42
Params size (MB): 56.42
Estimated Total Size (MB): 474.17
----------------------------------------------------------------
Learning rate: 0.0004
Augmentation: 1
Using distance weighted Dice
Reading data
Loaded data
calculating weights
Training model
Dice: 0.209830408994821
Loss: -0.6628134215485315
Epoch 1 loss = -0.6491201428199808
-------------
Dice: 0.3984209502691568
Loss: -0.8754205718685201
Epoch 2 loss = -0.7102147815469653
-------------
Dice: 0.4415745470338393
Loss: -0.782095750483894
Epoch 3 loss = -0.7161041996837594
-------------
Dice: 0.4121211177322446
Loss: -0.8925548550925166
Epoch 4 loss = -0.7254407023483267
-------------
Dice: 0.43984646378825293
Loss: -0.8454195595485356
Epoch 5 loss = -0.7338258111720036
-------------
Dice: 0.44197321076231494
Loss: -0.841420182702865
Epoch 6 loss = -0.7366249433159828
-------------
Dice: 0.43606770114870264
Loss: -0.8777788353558349
Epoch 7 loss = -0.7487591054942458
-------------
Dice: 0.43798808528999383
Loss: -0.8400054513426657
Epoch 8 loss = -0.7452922589704394
-------------
Dice: 0.4458212774003823
Loss: -0.8542450203984132
Epoch 9 loss = -0.7460357475252507
-------------
Dice: 0.45405476730499633
Loss: -0.8530986512046049
Epoch 10 loss = -0.7533004915217558
-------------
Dice: 0.4497195496111082
Loss: -0.862585758978905
Epoch 11 loss = -0.7536255932526971
-------------
Dice: 0.4570404121454472
Loss: -0.8484080427834759
Epoch 12 loss = -0.7591043020135294
-------------
Dice: 0.4578312803872839
Loss: -0.8213436579167003
Epoch 13 loss = -0.7674898453821576
-------------
Dice: 0.48022030233868906
Loss: -0.836393991693939
Epoch 14 loss = -0.7793398031406105
-------------
Dice: 0.5017367725020758
Loss: -0.793021953016087
Epoch 15 loss = -0.772599429232796
-------------
Dice: 0.4852588694056856
Loss: -0.8430015603488079
Epoch 16 loss = -0.776844125688076
-------------
Dice: 0.4931517088504436
Loss: -0.8261873593920663
Epoch 17 loss = -0.7892971093919914
-------------
Dice: 0.4964599463207508
Loss: -0.8113398829520262
Epoch 18 loss = -0.7927287937698808
-------------
Dice: 0.5014398398401652
Loss: -0.8294354308507258
Epoch 19 loss = -0.795375035641094
-------------
Dice: 0.5045598225539721
Loss: -0.8438877473252837
Epoch 20 loss = -0.7899189847769836
-------------
Dice: 0.5052253350725334
Loss: -0.8314354607438784
Epoch 21 loss = -0.8064458266180009
-------------
Dice: 0.4956669703950748
Loss: -0.8042785072430759
Epoch 22 loss = -0.8023271910759892
-------------
Dice: 0.5139174161635687
Loss: -0.8404204555327064
Epoch 23 loss = -0.7933650258060273
-------------
Dice: 0.498650487332647
Loss: -0.8161538232074352
Epoch 24 loss = -0.7998731428135458
-------------
Dice: 0.5119910173960394
Loss: -0.8428653673296632
Epoch 25 loss = -0.7938997625236213
-------------
Dice: 0.5150881025595957
Loss: -0.8200647447960627
Epoch 26 loss = -0.802639343837897
-------------
Dice: 0.5076043613222401
Loss: -0.8379586529198026
Epoch 27 loss = -0.8127960406988859
-------------
Dice: 0.5093901163794325
Loss: -0.8581617742386894
Epoch 28 loss = -0.8068642678360144
-------------
Dice: 0.5177579437150669
Loss: -0.8199841763463976
Epoch 29 loss = -0.8210582778217212
-------------
Dice: 0.5235962732717456
Loss: -0.8298004636493003
Epoch 30 loss = -0.817548662647605
-------------
Dice: 0.5278559778297608
Loss: -0.8104905143248144
Epoch 31 loss = -0.8118444545228947
-------------
Dice: 0.5261027301426633
Loss: -0.8462994775549373
Epoch 32 loss = -0.8219655593287826
-------------
Dice: 0.5256024937649217
Loss: -0.8274270827269504
Epoch 33 loss = -0.816562585712721
-------------
Dice: 0.5285934219942152
Loss: -0.8360503029050097
Epoch 34 loss = -0.8122077765557697
-------------
Dice: 0.5197524619012442
Loss: -0.8427721789784796
Epoch 35 loss = -0.821127434640354
-------------
Dice: 0.5231263816208873
Loss: -0.8480372644980426
Epoch 36 loss = -0.8429374799298421
-------------
Dice: 0.528831177393265
Loss: -0.8083054359260873
Epoch 37 loss = -0.8253247493890133
-------------
Dice: 0.5284137797310997
Loss: -0.8362407903048675
Epoch 38 loss = -0.833476708708331
-------------
Dice: 0.5208498206394009
Loss: -0.8452816792851764
Epoch 39 loss = -0.8335305557685205
-------------
Dice: 0.5309278891821193
Loss: -0.8225240500941189
Epoch 40 loss = -0.8295121867678487
-------------
