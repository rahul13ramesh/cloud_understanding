----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
  ConvTranspose2d-45          [-1, 512, 16, 26]       1,049,088
           Conv2d-46          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-47          [-1, 256, 17, 26]             512
             ReLU-48          [-1, 256, 17, 26]               0
           Conv2d-49          [-1, 256, 17, 26]         590,080
      BatchNorm2d-50          [-1, 256, 17, 26]             512
             ReLU-51          [-1, 256, 17, 26]               0
      double_conv-52          [-1, 256, 17, 26]               0
               up-53          [-1, 256, 17, 26]               0
  ConvTranspose2d-54          [-1, 256, 34, 52]         262,400
           Conv2d-55          [-1, 128, 35, 52]         589,952
      BatchNorm2d-56          [-1, 128, 35, 52]             256
             ReLU-57          [-1, 128, 35, 52]               0
           Conv2d-58          [-1, 128, 35, 52]         147,584
      BatchNorm2d-59          [-1, 128, 35, 52]             256
             ReLU-60          [-1, 128, 35, 52]               0
      double_conv-61          [-1, 128, 35, 52]               0
               up-62          [-1, 128, 35, 52]               0
  ConvTranspose2d-63         [-1, 128, 70, 104]          65,664
           Conv2d-64          [-1, 64, 70, 105]         147,520
      BatchNorm2d-65          [-1, 64, 70, 105]             128
             ReLU-66          [-1, 64, 70, 105]               0
           Conv2d-67          [-1, 64, 70, 105]          36,928
      BatchNorm2d-68          [-1, 64, 70, 105]             128
             ReLU-69          [-1, 64, 70, 105]               0
      double_conv-70          [-1, 64, 70, 105]               0
               up-71          [-1, 64, 70, 105]               0
  ConvTranspose2d-72         [-1, 64, 140, 210]          16,448
           Conv2d-73         [-1, 64, 140, 210]          73,792
      BatchNorm2d-74         [-1, 64, 140, 210]             128
             ReLU-75         [-1, 64, 140, 210]               0
           Conv2d-76         [-1, 64, 140, 210]          36,928
      BatchNorm2d-77         [-1, 64, 140, 210]             128
             ReLU-78         [-1, 64, 140, 210]               0
      double_conv-79         [-1, 64, 140, 210]               0
               up-80         [-1, 64, 140, 210]               0
           Conv2d-81          [-1, 4, 140, 210]             260
          outconv-82          [-1, 4, 140, 210]               0
================================================================
Total params: 14,789,124
Trainable params: 14,789,124
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 417.42
Params size (MB): 56.42
Estimated Total Size (MB): 474.17
----------------------------------------------------------------
Learning rate: 0.0004
Augmentation: 0
Using distance weighted BCE
Reading data
Loaded data
calculating weights
Training model
/tools/miniconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Dice: 0.24439964142776943
Loss: 0.09923036518973616
Epoch 1 loss = 0.10199138038791716
-------------
Dice: 0.2944492150925165
Loss: 0.22111531287736333
Epoch 2 loss = 0.09014017885550857
-------------
Dice: 0.300503276073186
Loss: 0.1940910987563471
Epoch 3 loss = 0.08749506178312004
-------------
Dice: 0.361030368830848
Loss: 0.19591306975915349
Epoch 4 loss = 0.08547009080648423
-------------
Dice: 0.44173171398711797
Loss: 0.1833302991704431
Epoch 5 loss = 0.0848082568993171
-------------
Dice: 0.41663772208184485
Loss: 0.18066466441402965
Epoch 6 loss = 0.08359513682002823
-------------
Dice: 0.40547499898705963
Loss: 0.19827174912044535
Epoch 7 loss = 0.08298978321254254
-------------
Dice: 0.3937400116067463
Loss: 0.20365726555842437
Epoch 8 loss = 0.08165329079143703
-------------
Dice: 0.3978000655852686
Loss: 0.20114463556747653
Epoch 9 loss = 0.0810649589356035
-------------
Dice: 0.4658737953552924
Loss: 0.16867979595889573
Epoch 10 loss = 0.0805457334065189
-------------
Dice: 0.4527116757312495
Loss: 0.1790833816758729
Epoch 11 loss = 0.0798159712087363
-------------
Dice: 0.4852737600433827
Loss: 0.15872432137327644
Epoch 12 loss = 0.07933837639478346
-------------
Dice: 0.4697513184486485
Loss: 0.17099369504119472
Epoch 13 loss = 0.07856538688763975
-------------
Dice: 0.4341388156486275
Loss: 0.1840699293723095
Epoch 14 loss = 0.07799341939079264
-------------
Dice: 0.43134778824176906
Loss: 0.1718195478616647
Epoch 15 loss = 0.07725021632077793
-------------
Dice: 0.4277489025764819
Loss: 0.1795282621178826
Epoch 16 loss = 0.07582971492782235
-------------
Dice: 0.4334412045164394
Loss: 0.18704346615987164
Epoch 17 loss = 0.07592317575899263
-------------
Dice: 0.46982629423861494
Loss: 0.1807711969220876
Epoch 18 loss = 0.07493455670773982
-------------
Dice: 0.4721334491038927
Loss: 0.16293183722328544
Epoch 19 loss = 0.07454931223454575
-------------
Dice: 0.41466310872594353
Loss: 0.19044192825711204
Epoch 20 loss = 0.07324878324133655
-------------
Dice: 0.4645701894535281
Loss: 0.18071284573762392
Epoch 21 loss = 0.07248823932372034
-------------
Dice: 0.4795167197221978
Loss: 0.16672037468521528
Epoch 22 loss = 0.07148445802430312
-------------
Dice: 0.4820262792370443
Loss: 0.17374243870896883
Epoch 23 loss = 0.07061127894558013
-------------
Dice: 0.44249696769968677
Loss: 0.20527222538279524
Epoch 24 loss = 0.06915628512389958
-------------
Dice: 0.47952730098878404
Loss: 0.17858071311754012
Epoch 25 loss = 0.0685017483153691
-------------
Dice: 0.49567970675839196
Loss: 0.1638578780438007
Epoch 26 loss = 0.0673131533091267
-------------
Dice: 0.4838873543938798
Loss: 0.1791537033778493
Epoch 27 loss = 0.06605323687195779
-------------
Dice: 0.44617097521355026
Loss: 0.19330857077740812
Epoch 28 loss = 0.06399403238669038
-------------
Dice: 0.48873962612630684
Loss: 0.18092449266093985
Epoch 29 loss = 0.06226141944217185
-------------
Dice: 0.40049774116143005
Loss: 0.22981539165511247
Epoch 30 loss = 0.060137192864591876
-------------
Dice: 0.4833409829958236
Loss: 0.1878998631027205
Epoch 31 loss = 0.0582874122960493
-------------
Dice: 0.4581104131887653
Loss: 0.21455828420959547
Epoch 32 loss = 0.055602997651634116
-------------
Dice: 0.49497608254995734
Loss: 0.19027586466971105
Epoch 33 loss = 0.05282465107428531
-------------
Dice: 0.4783122097917801
Loss: 0.20822872791405586
Epoch 34 loss = 0.0505542870843783
-------------
Dice: 0.4371357945565621
Loss: 0.2630557389041951
Epoch 35 loss = 0.04794343563572814
-------------
Dice: 0.48388947413547273
Loss: 0.203744230026956
Epoch 36 loss = 0.04561862118076533
-------------
Dice: 0.4251553773902295
Loss: 0.27423010675496257
Epoch 37 loss = 0.04279548527362446
-------------
Dice: 0.3933697393396618
Loss: 0.3093883360529458
Epoch 38 loss = 0.04046133935141067
-------------
Dice: 0.49764707975152667
Loss: 0.21954896150908407
Epoch 39 loss = 0.03857251385770117
-------------
Dice: 0.47055476491580894
Loss: 0.28457962438693873
Epoch 40 loss = 0.036031712853970624
-------------
Dice: 0.41639282103205744
Loss: 0.31482015257412893
Epoch 41 loss = 0.0347389075695537
-------------
Dice: 0.34878991249806945
Loss: 0.43834312480497334
Epoch 42 loss = 0.033000822219376766
-------------
Dice: 0.4857472102185935
Loss: 0.25545086915738663
Epoch 43 loss = 0.03171156574583923
-------------
Dice: 0.46793181233384873
Loss: 0.3229166689509012
Epoch 44 loss = 0.03036989481886849
-------------
Dice: 0.44589611475302515
Loss: 0.3231541226590995
Epoch 45 loss = 0.028483014922433842
-------------
Dice: 0.478863031958847
Loss: 0.2769984260439278
Epoch 46 loss = 0.027777273584312447
-------------
Dice: 0.46550350278184205
Loss: 0.3398277108024134
Epoch 47 loss = 0.026845053367627162
-------------
Dice: 0.47639160434171735
Loss: 0.2986999858661065
Epoch 48 loss = 0.025400957001838833
-------------
Dice: 0.4330223306880301
Loss: 0.37143391815796634
Epoch 49 loss = 0.0243984820577316
-------------
Dice: 0.45880652035366165
Loss: 0.3361272459949433
Epoch 50 loss = 0.02351538925509279
-------------
Dice: 0.4650071501998226
Loss: 0.33646477344385023
Epoch 51 loss = 0.023099588027301554
-------------
Dice: 0.41914408241945067
Loss: 0.4230130213899095
Epoch 52 loss = 0.022101817729805285
-------------
Dice: 0.49157903980961193
Loss: 0.27805200168796146
Epoch 53 loss = 0.02124125780072063
-------------
Dice: 0.4792015880039003
Loss: 0.3241706077408466
Epoch 54 loss = 0.020571648037293926
-------------
Dice: 0.4642161334817325
Loss: 0.3883164348523944
Epoch 55 loss = 0.02009707988239825
-------------
Dice: 0.45355278921517855
Loss: 0.43756079130453973
Epoch 56 loss = 0.01992460014143338
-------------
Dice: 0.4766476177312648
Loss: 0.34305985141969997
Epoch 57 loss = 0.018778221936663612
-------------
Dice: 0.47087149022731906
Loss: 0.3759009809477939
Epoch 58 loss = 0.018371459557674826
-------------
Dice: 0.4769954756817995
Loss: 0.41534582066673337
Epoch 59 loss = 0.017973477110111467
-------------
Dice: 0.43375291309962405
Loss: 0.4724735997025961
Epoch 60 loss = 0.01753177173086442
-------------
Dice: 0.4485957564335081
Loss: 0.4780368149218049
Epoch 61 loss = 0.017105167227952432
-------------
Dice: 0.4691169399339992
Loss: 0.4646690319833048
Epoch 62 loss = 0.016816149204193304
-------------
Dice: 0.4430270939651807
Loss: 0.4463648441190371
Epoch 63 loss = 0.016009057869280998
-------------
Dice: 0.45965297781972214
Loss: 0.4642463867485008
Epoch 64 loss = 0.015535435708006844
-------------
Dice: 0.4704644765728142
Loss: 0.4066666447927822
Epoch 65 loss = 0.01542616360122338
-------------
Dice: 0.46005670270500193
Loss: 0.4455661597564063
Epoch 66 loss = 0.014995810007482457
-------------
Dice: 0.46818929228368683
Loss: 0.44931661926646715
Epoch 67 loss = 0.014281181780000528
-------------
Dice: 0.47228267417382686
Loss: 0.46441282491086067
Epoch 68 loss = 0.014618440689907098
-------------
Dice: 0.46080564488336717
Loss: 0.45839055771129544
Epoch 69 loss = 0.014058986013988034
-------------
Dice: 0.45988467449438286
Loss: 0.4316629571450035
Epoch 70 loss = 0.013616661926886688
-------------
Dice: 0.464473435142903
Loss: 0.45647118981419993
Epoch 71 loss = 0.013227850566230093
-------------
Dice: 0.48347886209680896
Loss: 0.4308896039478737
Epoch 72 loss = 0.012886602571234107
-------------
Dice: 0.4212787232586018
Loss: 0.6022146391912817
Epoch 73 loss = 0.012578307416212435
-------------
Dice: 0.4531974947353388
Loss: 0.5184394906979318
Epoch 74 loss = 0.012626529115562639
-------------
Dice: 0.4746914722374066
Loss: 0.46601071958324386
Epoch 75 loss = 0.012636129755604391
-------------
Dice: 0.44633605485310984
Loss: 0.5070601329849578
Epoch 76 loss = 0.011552607943691935
-------------
Dice: 0.46145891191045163
Loss: 0.511691766466534
Epoch 77 loss = 0.011433571223557617
-------------
Dice: 0.4559869706511256
Loss: 0.5384558118657732
Epoch 78 loss = 0.011401589138743779
-------------
Dice: 0.46674476428895206
Loss: 0.5166152798769152
Epoch 79 loss = 0.011073366230508934
-------------
Dice: 0.46261720938309797
Loss: 0.5579145692632621
Epoch 80 loss = 0.011034035474876873
-------------
