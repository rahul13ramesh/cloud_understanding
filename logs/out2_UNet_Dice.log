----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 140, 210]           1,792
       BatchNorm2d-2         [-1, 64, 140, 210]             128
              ReLU-3         [-1, 64, 140, 210]               0
            Conv2d-4         [-1, 64, 140, 210]          36,928
       BatchNorm2d-5         [-1, 64, 140, 210]             128
              ReLU-6         [-1, 64, 140, 210]               0
       double_conv-7         [-1, 64, 140, 210]               0
            inconv-8         [-1, 64, 140, 210]               0
         MaxPool2d-9          [-1, 64, 70, 105]               0
           Conv2d-10         [-1, 128, 70, 105]          73,856
      BatchNorm2d-11         [-1, 128, 70, 105]             256
             ReLU-12         [-1, 128, 70, 105]               0
           Conv2d-13         [-1, 128, 70, 105]         147,584
      BatchNorm2d-14         [-1, 128, 70, 105]             256
             ReLU-15         [-1, 128, 70, 105]               0
      double_conv-16         [-1, 128, 70, 105]               0
             down-17         [-1, 128, 70, 105]               0
        MaxPool2d-18          [-1, 128, 35, 52]               0
           Conv2d-19          [-1, 256, 35, 52]         295,168
      BatchNorm2d-20          [-1, 256, 35, 52]             512
             ReLU-21          [-1, 256, 35, 52]               0
           Conv2d-22          [-1, 256, 35, 52]         590,080
      BatchNorm2d-23          [-1, 256, 35, 52]             512
             ReLU-24          [-1, 256, 35, 52]               0
      double_conv-25          [-1, 256, 35, 52]               0
             down-26          [-1, 256, 35, 52]               0
        MaxPool2d-27          [-1, 256, 17, 26]               0
           Conv2d-28          [-1, 512, 17, 26]       1,180,160
      BatchNorm2d-29          [-1, 512, 17, 26]           1,024
             ReLU-30          [-1, 512, 17, 26]               0
           Conv2d-31          [-1, 512, 17, 26]       2,359,808
      BatchNorm2d-32          [-1, 512, 17, 26]           1,024
             ReLU-33          [-1, 512, 17, 26]               0
      double_conv-34          [-1, 512, 17, 26]               0
             down-35          [-1, 512, 17, 26]               0
        MaxPool2d-36           [-1, 512, 8, 13]               0
           Conv2d-37           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-38           [-1, 512, 8, 13]           1,024
             ReLU-39           [-1, 512, 8, 13]               0
           Conv2d-40           [-1, 512, 8, 13]       2,359,808
      BatchNorm2d-41           [-1, 512, 8, 13]           1,024
             ReLU-42           [-1, 512, 8, 13]               0
      double_conv-43           [-1, 512, 8, 13]               0
             down-44           [-1, 512, 8, 13]               0
  ConvTranspose2d-45          [-1, 512, 16, 26]       1,049,088
           Conv2d-46          [-1, 256, 17, 26]       2,359,552
      BatchNorm2d-47          [-1, 256, 17, 26]             512
             ReLU-48          [-1, 256, 17, 26]               0
           Conv2d-49          [-1, 256, 17, 26]         590,080
      BatchNorm2d-50          [-1, 256, 17, 26]             512
             ReLU-51          [-1, 256, 17, 26]               0
      double_conv-52          [-1, 256, 17, 26]               0
               up-53          [-1, 256, 17, 26]               0
  ConvTranspose2d-54          [-1, 256, 34, 52]         262,400
           Conv2d-55          [-1, 128, 35, 52]         589,952
      BatchNorm2d-56          [-1, 128, 35, 52]             256
             ReLU-57          [-1, 128, 35, 52]               0
           Conv2d-58          [-1, 128, 35, 52]         147,584
      BatchNorm2d-59          [-1, 128, 35, 52]             256
             ReLU-60          [-1, 128, 35, 52]               0
      double_conv-61          [-1, 128, 35, 52]               0
               up-62          [-1, 128, 35, 52]               0
  ConvTranspose2d-63         [-1, 128, 70, 104]          65,664
           Conv2d-64          [-1, 64, 70, 105]         147,520
      BatchNorm2d-65          [-1, 64, 70, 105]             128
             ReLU-66          [-1, 64, 70, 105]               0
           Conv2d-67          [-1, 64, 70, 105]          36,928
      BatchNorm2d-68          [-1, 64, 70, 105]             128
             ReLU-69          [-1, 64, 70, 105]               0
      double_conv-70          [-1, 64, 70, 105]               0
               up-71          [-1, 64, 70, 105]               0
  ConvTranspose2d-72         [-1, 64, 140, 210]          16,448
           Conv2d-73         [-1, 64, 140, 210]          73,792
      BatchNorm2d-74         [-1, 64, 140, 210]             128
             ReLU-75         [-1, 64, 140, 210]               0
           Conv2d-76         [-1, 64, 140, 210]          36,928
      BatchNorm2d-77         [-1, 64, 140, 210]             128
             ReLU-78         [-1, 64, 140, 210]               0
      double_conv-79         [-1, 64, 140, 210]               0
               up-80         [-1, 64, 140, 210]               0
           Conv2d-81          [-1, 4, 140, 210]             260
          outconv-82          [-1, 4, 140, 210]               0
================================================================
Total params: 14,789,124
Trainable params: 14,789,124
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.34
Forward/backward pass size (MB): 417.42
Params size (MB): 56.42
Estimated Total Size (MB): 474.17
----------------------------------------------------------------
Learning rate: 0.008
Augmentation: 0
Using Dice loss
Reading data
Loaded data
Training model
Epoch 1 loss = -0.40927144333720206
Epoch 2 loss = -0.437985752671957
Epoch 3 loss = -0.44654862565298875
Epoch 4 loss = -0.4530993663519621
Epoch 5 loss = -0.45829709533602
Epoch 6 loss = -0.4685791190589468
Epoch 7 loss = -0.4925948734084765
Epoch 8 loss = -0.5032888143012921
Epoch 9 loss = -0.5109196895360947
Epoch 10 loss = -0.5177985393504302
Epoch 11 loss = -0.5229290005813042
Epoch 12 loss = -0.5271890014285843
Epoch 13 loss = -0.5330414615571499
Epoch 14 loss = -0.5384158805261056
Epoch 15 loss = -0.5435546916474898
Epoch 16 loss = -0.5491379056498409
Epoch 17 loss = -0.5554661436875661
Epoch 18 loss = -0.5626851595938206
Epoch 19 loss = -0.570710744249324
Epoch 20 loss = -0.5773545827468236
Epoch 21 loss = -0.5832328996310632
Epoch 22 loss = -0.5901450383042296
Epoch 23 loss = -0.596036353893578
Epoch 24 loss = -0.6038071970144908
Epoch 25 loss = -0.6116890201965968
Epoch 26 loss = -0.6172540085017681
Epoch 27 loss = -0.623440767750144
Epoch 28 loss = -0.6304616503665844
Epoch 29 loss = -0.6402388221025467
Epoch 30 loss = -0.6440492877364159
Epoch 31 loss = -0.6538316803673904
Epoch 32 loss = -0.6592962380995353
Epoch 33 loss = -0.6671725262701511
Epoch 34 loss = -0.6758438902099927
Epoch 35 loss = -0.6800572364777326
Epoch 36 loss = -0.6903664006044468
Epoch 37 loss = -0.697111096928517
Epoch 38 loss = -0.7033838569869598
Epoch 39 loss = -0.7104786120851835
Epoch 40 loss = -0.7136320542792479
Epoch 41 loss = -0.7196434943377972
Epoch 42 loss = -0.7273350610832373
Epoch 43 loss = -0.73336167636017
Epoch 44 loss = -0.7393169478823741
Epoch 45 loss = -0.7457770613332589
Epoch 46 loss = -0.7478396222243706
Epoch 47 loss = -0.7554593477894862
Epoch 48 loss = -0.7631510637203852
Epoch 49 loss = -0.7669954876353343
Epoch 50 loss = -0.7724645322312912
Epoch 51 loss = -0.776645363320907
Epoch 52 loss = -0.7800586215655009
Epoch 53 loss = -0.7847212727119526
Epoch 54 loss = -0.7898437733203173
Epoch 55 loss = -0.7936488956709703
Epoch 56 loss = -0.799372879266739
Epoch 57 loss = -0.8019021888077259
Epoch 58 loss = -0.8052723569174608
Epoch 59 loss = -0.8085787108540535
Epoch 60 loss = -0.8128090737263362
Epoch 61 loss = -0.8192298450072606
Epoch 62 loss = -0.8221044958631197
Epoch 63 loss = -0.8221369844675064
Epoch 64 loss = -0.8260107699533303
Epoch 65 loss = -0.8304635508855184
Epoch 66 loss = -0.8325368577738603
Epoch 67 loss = -0.8364678078889847
Epoch 68 loss = -0.8384313182036082
Epoch 69 loss = -0.8398785657187303
Epoch 70 loss = -0.8450370913743973
Epoch 71 loss = -0.8474713256458442
Epoch 72 loss = -0.8511638006071249
Epoch 73 loss = -0.8527697731554508
Epoch 74 loss = -0.8528680909176668
Epoch 75 loss = -0.85827263991038
Epoch 76 loss = -0.8613459151486555
Epoch 77 loss = -0.8628720963994662
Epoch 78 loss = -0.8681642854213715
Epoch 79 loss = -0.868714053183794
Epoch 80 loss = -0.8671752654512723
Dice: 0.43875290716423876
Loss: -0.4385538063150492
